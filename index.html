<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <title>VR Classroom Exposure Therapy</title>
  <script src="https://aframe.io/releases/1.4.2/aframe.min.js"></script>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
      -webkit-tap-highlight-color: transparent;
    }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      overflow: hidden;
      background: linear-gradient(135deg, #E8F4F8 0%, #F0F8FA 100%);
    }

    #vrOverlay {
      position: fixed;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      backdrop-filter: blur(8px);
      background: rgba(232, 244, 248, 0.85);
      display: flex;
      align-items: center;
      justify-content: center;
      z-index: 1000;
      transition: opacity 0.5s ease;
    }

    #vrOverlay.hidden {
      opacity: 0;
      visibility: hidden;
      pointer-events: none;
    }

    .overlay-card {
      max-width: 420px;
      width: 90%;
      background: white;
      border-radius: 16px;
      padding: 24px;
      box-shadow: 0 10px 40px rgba(19, 52, 59, 0.15);
    }

    .overlay-card h1 {
      font-size: 20px;
      color: rgba(19, 52, 59, 1);
      margin-bottom: 8px;
      font-weight: 600;
    }

    .overlay-card p {
      font-size: 13px;
      color: rgba(19, 52, 59, 0.7);
      margin-bottom: 20px;
      line-height: 1.5;
    }

    #conversationDisplay {
      max-height: 200px;
      overflow-y: auto;
      margin-bottom: 16px;
      padding: 12px;
      background: rgba(232, 244, 248, 0.5);
      border-radius: 8px;
    }

    .message {
      margin-bottom: 10px;
      padding: 8px;
      border-radius: 6px;
      font-size: 12px;
      line-height: 1.4;
    }

    .message.system {
      background: rgba(33, 128, 141, 0.1);
      border-left: 3px solid rgba(33, 128, 141, 1);
    }

    .message.user {
      background: rgba(94, 82, 64, 0.1);
      border-left: 3px solid rgba(94, 82, 64, 0.8);
    }

    .message strong {
      font-weight: 600;
      margin-right: 4px;
    }

    #voiceBtn {
      width: 100%;
      padding: 12px 20px;
      font-size: 14px;
      font-weight: 600;
      border: none;
      border-radius: 8px;
      background: #FF6B6B;
      color: white;
      cursor: pointer;
      transition: all 0.2s ease;
      margin-bottom: 12px;
    }

    #voiceBtn:hover {
      background: #EE5A5A;
    }

    #voiceBtn.listening {
      background: #51CF66;
      animation: pulse 1.5s infinite;
    }

    @keyframes pulse {
      0%, 100% { opacity: 1; }
      50% { opacity: 0.7; }
    }

    #voiceStatus {
      text-align: center;
      font-size: 12px;
      color: rgba(19, 52, 59, 0.6);
      margin-bottom: 16px;
      min-height: 20px;
    }

    #skipBtn {
      width: 100%;
      padding: 10px 20px;
      font-size: 13px;
      font-weight: 500;
      border: 1px solid rgba(94, 82, 64, 0.2);
      border-radius: 8px;
      background: rgba(94, 82, 64, 0.05);
      color: rgba(19, 52, 59, 1);
      cursor: pointer;
      transition: all 0.2s ease;
    }

    #skipBtn:hover {
      background: rgba(94, 82, 64, 0.1);
    }

    #manualOverlay {
      position: fixed;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      backdrop-filter: blur(8px);
      background: rgba(232, 244, 248, 0.85);
      display: none;
      align-items: center;
      justify-content: center;
      z-index: 1001;
    }

    #manualOverlay.active {
      display: flex;
    }

    .manual-form {
      max-width: 420px;
      width: 90%;
      background: white;
      border-radius: 16px;
      padding: 24px;
      box-shadow: 0 10px 40px rgba(19, 52, 59, 0.15);
      max-height: 80vh;
      overflow-y: auto;
    }

    .manual-form h2 {
      font-size: 18px;
      margin-bottom: 16px;
    }

    .form-group {
      margin-bottom: 16px;
    }

    .form-group label {
      display: block;
      font-size: 12px;
      font-weight: 600;
      margin-bottom: 6px;
    }

    .form-group input,
    .form-group textarea {
      width: 100%;
      padding: 10px 12px;
      font-size: 13px;
      border: 1px solid rgba(94, 82, 64, 0.2);
      border-radius: 6px;
      font-family: inherit;
    }

    .form-group textarea {
      resize: vertical;
      min-height: 80px;
    }

    .button-group {
      display: flex;
      gap: 10px;
      margin-top: 20px;
    }

    .button-group button {
      flex: 1;
      padding: 10px 20px;
      font-size: 13px;
      font-weight: 600;
      border: none;
      border-radius: 8px;
      cursor: pointer;
    }

    .btn-secondary {
      background: rgba(94, 82, 64, 0.1);
      color: rgba(19, 52, 59, 1);
    }

    .btn-primary {
      background: rgba(33, 128, 141, 1);
      color: white;
    }

    /* Mobile Touch Buttons Overlay */
    .mobile-controls {
      position: fixed;
      bottom: 20px;
      left: 50%;
      transform: translateX(-50%);
      z-index: 999;
      display: none;
      gap: 15px;
      flex-wrap: wrap;
      justify-content: center;
      max-width: 90%;
    }

    .mobile-controls.active {
      display: flex;
    }

    .mobile-btn {
      padding: 15px 30px;
      font-size: 16px;
      font-weight: 600;
      border: none;
      border-radius: 12px;
      background: rgba(33, 128, 141, 1);
      color: white;
      cursor: pointer;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
      transition: all 0.2s ease;
      touch-action: manipulation;
    }

    .mobile-btn:active {
      transform: scale(0.95);
      background: rgba(29, 116, 128, 1);
    }

    .mobile-btn.thumbs-up {
      background: #51CF66;
    }

    .mobile-btn.thumbs-up:active {
      background: #45b558;
    }

    .mobile-btn.thumbs-down {
      background: #FF6B6B;
    }

    .mobile-btn.thumbs-down:active {
      background: #e85555;
    }

    .mobile-btn.continue {
      background: rgba(33, 128, 141, 1);
    }

    .mobile-btn.continue:active {
      background: rgba(29, 116, 128, 1);
    }

    /* Voice indicator overlay */
    #voiceIndicator {
      position: fixed;
      top: 20px;
      right: 20px;
      z-index: 998;
      padding: 10px 20px;
      background: rgba(81, 207, 102, 0.9);
      color: white;
      border-radius: 8px;
      font-size: 14px;
      font-weight: 600;
      display: none;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
      animation: pulse 1.5s infinite;
    }

    #voiceIndicator.active {
      display: block;
    }
  </style>
</head>
<body>
  <div id="vrOverlay">
    <div class="overlay-card">
      <h1>üéì Virtual Lecter's Destressing Experience</h1>
      <p>We'll collect information through voice conversation before starting.</p>
      
      <div id="conversationDisplay">
        <div class="message system">
          <strong>System:</strong> Welcome to Virtual Lecter's Destressing Experience
        </div>
      </div>

      <button id="voiceBtn">üé§ Click to Start Voice Conversation</button>
      <div id="voiceStatus">Ready to begin</div>
      <button id="skipBtn">Skip Voice Mode (Use Manual Input)</button>
    </div>
  </div>

  <div id="manualOverlay">
    <div class="manual-form">
      <h2>Basic Information</h2>
      
      <div class="form-group">
        <label>Comfortable Languages</label>
        <input type="text" id="manualLanguages" placeholder="e.g., English, Hindi">
      </div>

      <div class="form-group">
        <label>Preferred Color</label>
        <input type="text" id="manualColor" placeholder="e.g., Blue, Green">
      </div>

      <div class="form-group">
        <label>Emergency Contact (Name and Phone)</label>
        <textarea id="manualEmergency" placeholder="e.g., John Doe, +91-1234567890"></textarea>
      </div>

      <div class="form-group">
        <label>Session Goals</label>
        <textarea id="manualGoals" placeholder="What would you like to work on today?"></textarea>
      </div>

      <div class="button-group">
        <button class="btn-secondary" onclick="closeManualInput()">Back</button>
        <button class="btn-primary" onclick="submitManualData()">Start VR</button>
      </div>
    </div>
  </div>

  <!-- Voice Listening Indicator -->
  <div id="voiceIndicator">üé§ Listening...</div>

  <!-- Mobile Touch Controls -->
  <div id="mobileControls" class="mobile-controls">
    <button id="mobileSessionBtn" class="mobile-btn" style="display: none;">Begin Guided Session</button>
    <button id="mobileThumbsUp" class="mobile-btn thumbs-up" style="display: none;">üëç Yes</button>
    <button id="mobileThumbsDown" class="mobile-btn thumbs-down" style="display: none;">üëé Not Really</button>
    <button id="mobileContinue" class="mobile-btn continue" style="display: none;">Continue to Phase 2</button>
  </div>

  <a-scene>
    <a-sky color="#E8F4F8"></a-sky>
    <a-entity light="type: ambient; color: #BBB; intensity: 0.8"></a-entity>
    <a-entity light="type: directional; color: #FFF; intensity: 0.6" position="5 10 7"></a-entity>
    <a-entity light="type: point; color: #FFF; intensity: 0.4" position="0 3.5 0"></a-entity>

    <a-entity 
      gltf-model="https://raw.githubusercontent.com/o-meiuqer-o/classroom_VR4ET/main/assets/models/classroom.glb"
      position="0 0 0"
      rotation="0 180 0"
      scale="1.15 1.15 1.15"
      shadow="cast: true; receive: true">
    </a-entity>

    <a-entity id="rig" position="-0.5 0.6 1.8">
      <a-camera look-controls wasd-controls>
        <a-cursor color="#21808D" raycaster="objects: .clickable"></a-cursor>
      </a-camera>
    </a-entity>

    <!-- Session Start Button -->
    <a-entity id="sessionButton" 
              position="0 1.6 -2"
              visible="false"
              class="clickable">
      <a-plane width="1.5" height="0.5" color="#21808D" opacity="0.95"></a-plane>
      <a-text value="Begin Guided Session" 
              position="0 0 0.01" 
              align="center"
              width="2.5"
              color="#FFFFFF"
              font="roboto"
              wrap-count="30"></a-text>
    </a-entity>

    <!-- Feedback Buttons (hidden initially) -->
    <a-entity id="feedbackButtons" visible="false">
      <a-entity id="thumbsUpBtn" position="-0.5 1.5 -1.8" class="clickable">
        <a-plane width="0.6" height="0.4" color="#51CF66"></a-plane>
        <a-text value="üëç Yes" position="0 0 0.01" align="center" width="1.2" color="#FFF"></a-text>
      </a-entity>
      
      <a-entity id="thumbsDownBtn" position="0.5 1.5 -1.8" class="clickable">
        <a-plane width="0.6" height="0.4" color="#FF6B6B"></a-plane>
        <a-text value="üëé Not Really" position="0 0 0.01" align="center" width="1.2" color="#FFF"></a-text>
      </a-entity>
    </a-entity>

    <!-- Continue Button (hidden initially) -->
    <a-entity id="continueButton" 
              position="0 1.6 -2"
              visible="false"
              class="clickable">
      <a-plane width="1.8" height="0.5" color="#21808D" opacity="0.95"></a-plane>
      <a-text value="Continue to Phase 2" 
              position="0 0 0.01" 
              align="center"
              width="2.8"
              color="#FFFFFF"
              font="roboto"
              wrap-count="30"></a-text>
    </a-entity>

    <!-- Text Display (for showing numbers 5-4-3-2-1) -->
    <a-text id="displayText" 
            position="0 2 -1.8" 
            align="center" 
            width="3"
            color="#21808D"
            visible="false"></a-text>
  </a-scene>

  <script>
    console.log('VR Classroom v31 - Voice Response + Confirmation - Initialized');

    const sessionData = {
      comfortableLanguages: '',
      preferredColor: '',
      emergencyContact: '',
      goals: '',
      startTime: new Date()
    };

    let recognition = null;
    let synthesis = window.speechSynthesis;
    let voices = [];
    let recognitionTimeout = null;
    let isSpeaking = false;
    let isListeningForResponse = false;
    let responseCallback = null;

    // Grounding exercise state
    let groundingStep = 0;
    const groundingSteps = [
      { text: "We're starting with a grounding exercise. This isn't about relaxation yet‚Äîthis is about bringing you fully into the present moment so your brain knows where you are and what's real.", duration: 6000 },
      { text: "It's called 5-4-3-2-1. We're going to use your senses. I'll guide you.", duration: 4000, showNumbers: true },
      { text: "Look around. FIVE things you can SEE right now. Name them out loud or in your head.", duration: 8000 },
      { text: "FOUR things you can TOUCH or feel right now. Actually reach out and touch them if you can. The chair under you? Your shirt? The floor under your feet? Your hair?", duration: 8000 },
      { text: "THREE things you can HEAR right now. Could be my voice, background sounds, maybe your own breathing.", duration: 8000 },
      { text: "TWO things you can SMELL. If you can't smell anything obvious, that's fine‚Äîjust notice that.", duration: 5000 },
      { text: "ONE thing you can TASTE. Maybe you brushed your teeth earlier, or had water. Or maybe you don't taste anything‚Äîthat's fine too.", duration: 5000 },
      { text: "Good. Now take one deep breath in... and out.", duration: 4000 },
      { text: "Small check-in - do you feel a little more here? A little more solid? You can say yes or no, or tap a button.", duration: 3000, showFeedback: true },
      { text: "Good. That grounding technique! You can use that in the exam hall if you start feeling spaced out. We'll come back to that.", duration: 5000, positive: true },
      { text: "That's okay. Sometimes it takes a few tries. We can do it again later if needed.", duration: 4000, negative: true }
    ];

    if (synthesis) {
      synthesis.onvoiceschanged = () => {
        voices = synthesis.getVoices();
        console.log('Voices loaded:', voices.length);
      };
    }

    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      recognition = new SpeechRecognition();
      recognition.continuous = false;
      recognition.interimResults = true;
      recognition.lang = 'en-US';
      console.log('Speech Recognition initialized');
    }

    let conversationState = {
      step: 'welcome',
      responses: {},
      isListening: false
    };

    const conversationFlow = {
      welcome: {
        text: "Welcome to Virtual Lecter's Destressing Experience. We'll practice safely in a virtual classroom. Before we begin, I will ask a few questions. Are you ready to start?",
        nextStep: 'language'
      },
      language: {
        text: "Which languages are you comfortable with? Feel free to mention one or more.",
        nextStep: 'color',
        saveAs: 'comfortableLanguages'
      },
      color: {
        text: "What is your preferred color for the virtual environment?",
        nextStep: 'emergencyContact',
        saveAs: 'preferredColor'
      },
      emergencyContact: {
        text: "For your safety, please provide the name and phone number of an emergency contact person.",
        nextStep: 'goals',
        saveAs: 'emergencyContact'
      },
      goals: {
        text: "What would you like to work on today?",
        nextStep: 'complete',
        saveAs: 'goals'
      },
      complete: {
        text: "Thank you. We're ready to begin. Are you ready to enter the virtual environment?",
        nextStep: 'vr'
      }
    };

    function speak(text, callback) {
      console.log('Speaking:', text.substring(0, 40) + '...');
      
      if (!synthesis) {
        console.log('Speech synthesis not available');
        if (callback) setTimeout(callback, 500);
        return;
      }

      synthesis.cancel();
      isSpeaking = true;

      const utterance = new SpeechSynthesisUtterance(text);
      utterance.rate = 0.85;
      utterance.lang = 'en-US';

      if (voices.length > 0) {
        const voice = voices.find(v => v.lang.includes('en') && v.name.includes('Female')) || voices[0];
        utterance.voice = voice;
      }

      let completed = false;
      const finishSpeech = () => {
        if (!completed) {
          completed = true;
          isSpeaking = false;
          console.log('Speech completed');
          if (callback) {
            setTimeout(callback, 1000);
          }
        }
      };

      utterance.onend = finishSpeech;
      utterance.onerror = (e) => {
        console.error('Speech error:', e);
        finishSpeech();
      };

      const wordCount = text.split(' ').length;
      const estimatedDuration = (wordCount * 150) + 2000;
      setTimeout(finishSpeech, estimatedDuration);

      addMessage('System', text);
      synthesis.speak(utterance);
    }

    function addMessage(sender, text) {
      const display = document.getElementById('conversationDisplay');
      const msg = document.createElement('div');
      msg.className = `message ${sender.toLowerCase()}`;
      msg.innerHTML = `<strong>${sender}:</strong> ${text}`;
      display.appendChild(msg);
      display.scrollTop = display.scrollHeight;
    }

    async function requestMicPermission() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        stream.getTracks().forEach(track => track.stop());
        console.log('Microphone permission granted');
        return true;
      } catch (error) {
        console.error('Microphone permission denied:', error);
        alert('Please allow microphone access.');
        return false;
      }
    }

    // Voice response listener for yes/no questions
    function startVoiceResponseListener(callback) {
      if (!recognition) {
        console.log('Voice recognition not available');
        return;
      }

      console.log('Starting voice response listener');
      isListeningForResponse = true;
      responseCallback = callback;

      document.getElementById('voiceIndicator').classList.add('active');

      if (recognitionTimeout) clearTimeout(recognitionTimeout);
      recognitionTimeout = setTimeout(() => {
        console.log('Voice response timeout');
        stopVoiceResponseListener();
      }, 30000); // 30 second timeout for response

      recognition.onresult = (event) => {
        let finalTranscript = '';
        
        for (let i = event.resultIndex; i < event.results.length; i++) {
          if (event.results[i].isFinal) {
            finalTranscript += event.results[i][0].transcript;
          }
        }

        if (finalTranscript) {
          const transcript = finalTranscript.trim().toLowerCase();
          console.log('Voice response:', transcript);

          // Check for yes/no/continue responses
          const isYes = transcript.includes('yes') || transcript.includes('yeah') || 
                        transcript.includes('yep') || transcript.includes('sure') ||
                        transcript.includes('okay') || transcript.includes('ok');
          
          const isNo = transcript.includes('no') || transcript.includes('nope') || 
                       transcript.includes('not really') || transcript.includes('nah');

          const isContinue = transcript.includes('continue') || transcript.includes('next') ||
                             transcript.includes('proceed') || transcript.includes('go ahead');

          if (isYes || isNo || isContinue) {
            if (recognitionTimeout) clearTimeout(recognitionTimeout);
            stopVoiceResponseListener();
            
            if (responseCallback) {
              if (isContinue) {
                responseCallback('continue');
              } else {
                responseCallback(isYes);
              }
            }
          }
        }
      };

      recognition.onerror = (event) => {
        console.error('Voice response error:', event.error);
        stopVoiceResponseListener();
      };

      recognition.onend = () => {
        if (isListeningForResponse) {
          setTimeout(() => {
            try {
              recognition.start();
            } catch (e) {
              console.error('Voice response restart failed:', e);
            }
          }, 100);
        }
      };

      try {
        recognition.start();
      } catch (error) {
        console.error('Failed to start voice response:', error);
      }
    }

    function stopVoiceResponseListener() {
      isListeningForResponse = false;
      responseCallback = null;
      document.getElementById('voiceIndicator').classList.remove('active');
      
      if (recognition) {
        try {
          recognition.stop();
        } catch (e) {
          console.log('Recognition already stopped');
        }
      }
    }

    async function startConversation() {
      const voiceBtn = document.getElementById('voiceBtn');
      const statusEl = document.getElementById('voiceStatus');

      const currentStep = conversationState.step;
      const currentFlow = conversationFlow[currentStep];

      if (currentStep !== 'welcome' && Object.keys(conversationState.responses).length > 0) {
        console.log('Resuming conversation');
        voiceBtn.textContent = 'üîä Resuming...';
        speak("Let's continue. " + currentFlow.text, () => {
          startListening(currentStep);
        });
        return;
      }

      voiceBtn.textContent = 'üé§ Requesting microphone...';
      const hasPermission = await requestMicPermission();
      
      if (!hasPermission) {
        voiceBtn.textContent = 'üé§ Click to Start';
        statusEl.textContent = '‚ùå Microphone denied';
        return;
      }

      voiceBtn.textContent = 'üîä Speaking...';
      speak(conversationFlow.welcome.text, () => {
        startListening('welcome');
      });
    }

    function startListening(step) {
      if (!recognition) {
        alert('Voice recognition not supported. Use manual input.');
        return;
      }

      console.log('Starting to listen for:', step);
      
      const voiceBtn = document.getElementById('voiceBtn');
      const statusEl = document.getElementById('voiceStatus');

      conversationState.step = step;
      conversationState.isListening = true;

      voiceBtn.textContent = 'üé§ Listening...';
      voiceBtn.classList.add('listening');
      statusEl.textContent = 'üî¥ LISTENING';
      statusEl.style.color = '#FF6B6B';
      statusEl.style.fontWeight = 'bold';

      if (recognitionTimeout) clearTimeout(recognitionTimeout);

      recognitionTimeout = setTimeout(() => {
        console.log('Recognition timeout');
        recognition.stop();
      }, 60000);

      recognition.onresult = (event) => {
        let finalTranscript = '';
        
        for (let i = event.resultIndex; i < event.results.length; i++) {
          if (event.results[i].isFinal) {
            finalTranscript += event.results[i][0].transcript;
          } else {
            statusEl.textContent = `üìù "${event.results[i][0].transcript}"`;
          }
        }

        if (finalTranscript) {
          const transcript = finalTranscript.trim();
          console.log('Final result:', transcript);

          if (recognitionTimeout) clearTimeout(recognitionTimeout);
          conversationState.isListening = false;

          voiceBtn.classList.remove('listening');
          voiceBtn.textContent = '‚úÖ Got it!';
          statusEl.textContent = `You said: "${transcript}"`;
          statusEl.style.color = '#51CF66';

          addMessage('You', transcript);
          processResponse(transcript, step);
        }
      };

      recognition.onerror = (event) => {
        console.error('Recognition error:', event.error);
        voiceBtn.classList.remove('listening');
        voiceBtn.textContent = 'üîÑ Click to Resume';
        statusEl.textContent = 'Error: ' + event.error;
        conversationState.isListening = false;
      };

      recognition.onend = () => {
        console.log('Recognition ended');
        if (conversationState.isListening) {
          setTimeout(() => {
            try {
              recognition.start();
            } catch (e) {
              console.error('Restart failed:', e);
            }
          }, 100);
        }
      };

      try {
        recognition.start();
      } catch (error) {
        console.error('Failed to start:', error);
      }
    }

    function processResponse(transcript, step) {
      const currentFlow = conversationFlow[step];
      
      if (currentFlow.saveAs) {
        conversationState.responses[currentFlow.saveAs] = transcript;
      }

      if (currentFlow.nextStep === 'vr') {
        completeConversation();
      } else {
        setTimeout(() => continueConversation(currentFlow.nextStep), 1000);
      }
    }

    function continueConversation(nextStep) {
      const nextFlow = conversationFlow[nextStep];
      const voiceBtn = document.getElementById('voiceBtn');
      const statusEl = document.getElementById('voiceStatus');

      voiceBtn.textContent = 'üîä Speaking...';
      voiceBtn.classList.remove('listening');
      statusEl.textContent = 'Speaking';
      statusEl.style.color = '';
      statusEl.style.fontWeight = '';

      speak(nextFlow.text, () => {
        startListening(nextStep);
      });
    }

    function completeConversation() {
      Object.assign(sessionData, conversationState.responses);
      console.log('Conversation complete:', sessionData);

      const voiceBtn = document.getElementById('voiceBtn');
      voiceBtn.textContent = '‚úÖ Complete';

      setTimeout(() => {
        document.getElementById('vrOverlay').classList.add('hidden');
        
        // Show VR button AND mobile button
        document.getElementById('sessionButton').setAttribute('visible', 'true');
        showMobileSessionButton();
        
        speak('Welcome to the virtual classroom. When ready, tap or click the Begin Guided Session button.');
      }, 2000);
    }

    function showManualInput() {
      document.getElementById('manualOverlay').classList.add('active');
    }

    function closeManualInput() {
      document.getElementById('manualOverlay').classList.remove('active');
    }

    function submitManualData() {
      sessionData.comfortableLanguages = document.getElementById('manualLanguages').value;
      sessionData.preferredColor = document.getElementById('manualColor').value;
      sessionData.emergencyContact = document.getElementById('manualEmergency').value;
      sessionData.goals = document.getElementById('manualGoals').value;

      console.log('Manual data submitted:', sessionData);
      closeManualInput();
      document.getElementById('vrOverlay').classList.add('hidden');
      
      // Show VR button AND mobile button
      document.getElementById('sessionButton').setAttribute('visible', 'true');
      showMobileSessionButton();
      
      speak('Welcome to the virtual classroom. When ready, tap or click the Begin Guided Session button.');
    }

    // Mobile Control Functions
    function showMobileSessionButton() {
      document.getElementById('mobileControls').classList.add('active');
      document.getElementById('mobileSessionBtn').style.display = 'block';
    }

    function hideMobileSessionButton() {
      document.getElementById('mobileSessionBtn').style.display = 'none';
    }

    function showMobileFeedbackButtons() {
      document.getElementById('mobileControls').classList.add('active');
      document.getElementById('mobileThumbsUp').style.display = 'block';
      document.getElementById('mobileThumbsDown').style.display = 'block';
    }

    function hideMobileFeedbackButtons() {
      document.getElementById('mobileThumbsUp').style.display = 'none';
      document.getElementById('mobileThumbsDown').style.display = 'none';
    }

    function showMobileContinueButton() {
      document.getElementById('mobileControls').classList.add('active');
      document.getElementById('mobileContinue').style.display = 'block';
    }

    function hideMobileContinueButton() {
      document.getElementById('mobileContinue').style.display = 'none';
    }

    // Grounding Exercise Functions
    function startGuidedSession() {
      console.log('Starting guided session - Phase 1: Grounding');
      groundingStep = 0;
      document.getElementById('sessionButton').setAttribute('visible', 'false');
      hideMobileSessionButton();
      nextGroundingStep();
    }

    function nextGroundingStep() {
      if (groundingStep >= groundingSteps.length - 2) return;

      const step = groundingSteps[groundingStep];
      console.log('Grounding step', groundingStep, ':', step.text.substring(0, 40));

      // Show numbers for step 1
      if (step.showNumbers) {
        document.getElementById('displayText').setAttribute('value', '5  4  3  2  1');
        document.getElementById('displayText').setAttribute('visible', 'true');
        setTimeout(() => {
          document.getElementById('displayText').setAttribute('visible', 'false');
        }, 3000);
      }

      // Show feedback buttons for step 8
      if (step.showFeedback) {
        speak(step.text, () => {
          document.getElementById('feedbackButtons').setAttribute('visible', 'true');
          showMobileFeedbackButtons();
          
          // Start voice listener for yes/no
          startVoiceResponseListener((response) => {
            handleFeedback(response);
          });
        });
        return;
      }

      speak(step.text, () => {
        setTimeout(() => {
          groundingStep++;
          nextGroundingStep();
        }, step.duration);
      });
    }

    function handleFeedback(isPositive) {
      stopVoiceResponseListener();
      document.getElementById('feedbackButtons').setAttribute('visible', 'false');
      hideMobileFeedbackButtons();
      
      const responseStep = isPositive ? 9 : 10;
      speak(groundingSteps[responseStep].text, () => {
        console.log('Grounding exercise complete');
        askToContinue();
      });
    }

    function askToContinue() {
      speak('Would you like to continue to Phase 2? You can say continue, or tap the button.', () => {
        document.getElementById('continueButton').setAttribute('visible', 'true');
        showMobileContinueButton();
        
        // Start voice listener for continue
        startVoiceResponseListener((response) => {
          if (response === 'continue' || response === true) {
            continueToPhase2();
          }
        });
      });
    }

    function continueToPhase2() {
      stopVoiceResponseListener();
      document.getElementById('continueButton').setAttribute('visible', 'false');
      hideMobileContinueButton();
      
      speak('Great! Phase 2 will be implemented in the next update. For now, you can explore the virtual classroom.');
    }

    // Event Listeners
    document.getElementById('voiceBtn').addEventListener('click', startConversation);
    document.getElementById('skipBtn').addEventListener('click', showManualInput);

    // Mobile button listeners
    document.getElementById('mobileSessionBtn').addEventListener('click', startGuidedSession);
    document.getElementById('mobileSessionBtn').addEventListener('touchend', (e) => {
      e.preventDefault();
      startGuidedSession();
    });

    document.getElementById('mobileThumbsUp').addEventListener('click', () => handleFeedback(true));
    document.getElementById('mobileThumbsUp').addEventListener('touchend', (e) => {
      e.preventDefault();
      handleFeedback(true);
    });

    document.getElementById('mobileThumbsDown').addEventListener('click', () => handleFeedback(false));
    document.getElementById('mobileThumbsDown').addEventListener('touchend', (e) => {
      e.preventDefault();
      handleFeedback(false);
    });

    document.getElementById('mobileContinue').addEventListener('click', () => continueToPhase2());
    document.getElementById('mobileContinue').addEventListener('touchend', (e) => {
      e.preventDefault();
      continueToPhase2();
    });

    // VR button listeners (for desktop)
    document.addEventListener('DOMContentLoaded', () => {
      const sessionBtn = document.getElementById('sessionButton');
      sessionBtn.addEventListener('click', startGuidedSession);

      document.getElementById('thumbsUpBtn').addEventListener('click', () => handleFeedback(true));
      document.getElementById('thumbsDownBtn').addEventListener('click', () => handleFeedback(false));
      
      document.getElementById('continueButton').addEventListener('click', () => continueToPhase2());
    });
  </script>
</body>
</html>

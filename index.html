<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <title>VR Classroom Exposure Therapy</title>
  <script src="https://aframe.io/releases/1.4.2/aframe.min.js"></script>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
      -webkit-tap-highlight-color: transparent;
    }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      overflow: hidden;
      background: linear-gradient(135deg, #E8F4F8 0%, #F0F8FA 100%);
    }

    #vrOverlay {
      position: fixed;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      backdrop-filter: blur(8px);
      background: rgba(232, 244, 248, 0.85);
      display: flex;
      align-items: center;
      justify-content: center;
      z-index: 1000;
      transition: opacity 0.5s ease;
    }

    #vrOverlay.hidden {
      opacity: 0;
      visibility: hidden;
      pointer-events: none;
    }

    .overlay-card {
      max-width: 420px;
      width: 90%;
      background: white;
      border-radius: 16px;
      padding: 24px;
      box-shadow: 0 10px 40px rgba(19, 52, 59, 0.15);
    }

    .overlay-card h1 {
      font-size: 20px;
      color: rgba(19, 52, 59, 1);
      margin-bottom: 8px;
      font-weight: 600;
    }

    .overlay-card p {
      font-size: 13px;
      color: rgba(19, 52, 59, 0.7);
      margin-bottom: 20px;
      line-height: 1.5;
    }

    #conversationDisplay {
      max-height: 200px;
      overflow-y: auto;
      margin-bottom: 16px;
      padding: 12px;
      background: rgba(232, 244, 248, 0.5);
      border-radius: 8px;
    }

    .message {
      margin-bottom: 10px;
      padding: 8px;
      border-radius: 6px;
      font-size: 12px;
      line-height: 1.4;
    }

    .message.system {
      background: rgba(33, 128, 141, 0.1);
      border-left: 3px solid rgba(33, 128, 141, 1);
    }

    .message.user {
      background: rgba(94, 82, 64, 0.1);
      border-left: 3px solid rgba(94, 82, 64, 0.8);
    }

    .message.live {
      background: rgba(94, 82, 64, 0.05);
      border-left: 3px solid rgba(94, 82, 64, 0.4);
      font-style: italic;
    }

    .message strong {
      font-weight: 600;
      margin-right: 4px;
    }

    #voiceBtn {
      width: 100%;
      padding: 12px 20px;
      font-size: 14px;
      font-weight: 600;
      border: none;
      border-radius: 8px;
      background: #FF6B6B;
      color: white;
      cursor: pointer;
      transition: all 0.2s ease;
      margin-bottom: 12px;
    }

    #voiceBtn:hover {
      background: #EE5A5A;
    }

    #voiceBtn.listening {
      background: #51CF66;
      animation: pulse 1.5s infinite;
    }

    @keyframes pulse {
      0%, 100% { opacity: 1; }
      50% { opacity: 0.7; }
    }

    #voiceStatus {
      text-align: center;
      font-size: 12px;
      color: rgba(19, 52, 59, 0.6);
      margin-bottom: 16px;
      min-height: 20px;
    }

    #skipBtn {
      width: 100%;
      padding: 10px 20px;
      font-size: 13px;
      font-weight: 500;
      border: 1px solid rgba(94, 82, 64, 0.2);
      border-radius: 8px;
      background: rgba(94, 82, 64, 0.05);
      color: rgba(19, 52, 59, 1);
      cursor: pointer;
      transition: all 0.2s ease;
    }

    #skipBtn:hover {
      background: rgba(94, 82, 64, 0.1);
    }

    #manualOverlay {
      position: fixed;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      backdrop-filter: blur(8px);
      background: rgba(232, 244, 248, 0.85);
      display: none;
      align-items: center;
      justify-content: center;
      z-index: 1001;
    }

    #manualOverlay.active {
      display: flex;
    }

    .manual-form {
      max-width: 420px;
      width: 90%;
      background: white;
      border-radius: 16px;
      padding: 24px;
      box-shadow: 0 10px 40px rgba(19, 52, 59, 0.15);
      max-height: 80vh;
      overflow-y: auto;
    }

    .manual-form h2 {
      font-size: 18px;
      margin-bottom: 16px;
    }

    .form-group {
      margin-bottom: 16px;
    }

    .form-group label {
      display: block;
      font-size: 12px;
      font-weight: 600;
      margin-bottom: 6px;
    }

    .form-group input,
    .form-group textarea {
      width: 100%;
      padding: 10px 12px;
      font-size: 13px;
      border: 1px solid rgba(94, 82, 64, 0.2);
      border-radius: 6px;
      font-family: inherit;
    }

    .form-group textarea {
      resize: vertical;
      min-height: 80px;
    }

    .button-group {
      display: flex;
      gap: 10px;
      margin-top: 20px;
    }

    .button-group button {
      flex: 1;
      padding: 10px 20px;
      font-size: 13px;
      font-weight: 600;
      border: none;
      border-radius: 8px;
      cursor: pointer;
    }

    .btn-secondary {
      background: rgba(94, 82, 64, 0.1);
      color: rgba(19, 52, 59, 1);
    }

    .btn-primary {
      background: rgba(33, 128, 141, 1);
      color: white;
    }

    /* Mobile Touch Buttons Overlay */
    .mobile-controls {
      position: fixed;
      bottom: 20px;
      left: 50%;
      transform: translateX(-50%);
      z-index: 999;
      display: none;
      gap: 15px;
      flex-wrap: wrap;
      justify-content: center;
      max-width: 90%;
    }

    .mobile-controls.active {
      display: flex;
    }

    .mobile-btn {
      padding: 15px 30px;
      font-size: 16px;
      font-weight: 600;
      border: none;
      border-radius: 12px;
      background: rgba(33, 128, 141, 1);
      color: white;
      cursor: pointer;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
      transition: all 0.2s ease;
      touch-action: manipulation;
    }

    .mobile-btn:active {
      transform: scale(0.95);
      background: rgba(29, 116, 128, 1);
    }

    .mobile-btn.thumbs-up {
      background: #51CF66;
    }

    .mobile-btn.thumbs-up:active {
      background: #45b558;
    }

    .mobile-btn.thumbs-down {
      background: #FF6B6B;
    }

    .mobile-btn.thumbs-down:active {
      background: #e85555;
    }

    /* Voice indicator overlay */
    #voiceIndicator {
      position: fixed;
      top: 20px;
      right: 20px;
      z-index: 998;
      padding: 10px 20px;
      background: rgba(81, 207, 102, 0.9);
      color: white;
      border-radius: 8px;
      font-size: 14px;
      font-weight: 600;
      display: none;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
      animation: pulse 1.5s infinite;
    }

    #voiceIndicator.active {
      display: block;
    }

    /* Breathing circle - FIXED ANIMATION */
    #breathingCircle {
      position: fixed;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%) scale(1);
      width: 120px;
      height: 120px;
      border-radius: 50%;
      background: radial-gradient(circle, rgba(81, 207, 102, 0.4), rgba(81, 207, 102, 0.1));
      border: 3px solid #51CF66;
      display: none;
      z-index: 997;
      box-shadow: 0 0 30px rgba(81, 207, 102, 0.3);
    }

    #breathingCircle.active {
      display: block;
    }

    /* Breathing animations - maintains state */
    #breathingCircle.inhale {
      animation: breatheIn 4s ease-in-out forwards;
    }

    #breathingCircle.exhale {
      animation: breatheOut 6s ease-in-out forwards;
    }

    #breathingCircle.hold-full {
      transform: translate(-50%, -50%) scale(1.8);
      border-color: #45b558;
      box-shadow: 0 0 50px rgba(81, 207, 102, 0.5);
    }

    #breathingCircle.hold-empty {
      transform: translate(-50%, -50%) scale(1);
      border-color: #51CF66;
      box-shadow: 0 0 30px rgba(81, 207, 102, 0.3);
    }

    @keyframes breatheIn {
      from { 
        transform: translate(-50%, -50%) scale(1); 
        border-color: #51CF66;
        box-shadow: 0 0 30px rgba(81, 207, 102, 0.3);
      }
      to { 
        transform: translate(-50%, -50%) scale(1.8); 
        border-color: #45b558;
        box-shadow: 0 0 50px rgba(81, 207, 102, 0.5);
      }
    }

    @keyframes breatheOut {
      from { 
        transform: translate(-50%, -50%) scale(1.8); 
        border-color: #45b558;
        box-shadow: 0 0 50px rgba(81, 207, 102, 0.5);
      }
      to { 
        transform: translate(-50%, -50%) scale(1); 
        border-color: #51CF66;
        box-shadow: 0 0 30px rgba(81, 207, 102, 0.3);
      }
    }

    /* Breathing instruction text */
    #breathingText {
      position: fixed;
      top: 40%;
      left: 50%;
      transform: translate(-50%, -50%);
      font-size: 24px;
      font-weight: 600;
      color: #21808D;
      display: none;
      z-index: 998;
      text-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }

    #breathingText.active {
      display: block;
    }
  </style>
</head>
<body>
  <div id="vrOverlay">
    <div class="overlay-card">
      <h1>üéì Virtual Lecter's Destressing Experience</h1>
      <p>We'll collect information through voice conversation before starting.</p>
      
      <div id="conversationDisplay">
        <div class="message system">
          <strong>System:</strong> Welcome to Virtual Lecter's Destressing Experience
        </div>
      </div>

      <button id="voiceBtn">üé§ Click to Start Voice Conversation</button>
      <div id="voiceStatus">Ready to begin</div>
      <button id="skipBtn">Skip Voice Mode (Use Manual Input)</button>
    </div>
  </div>

  <div id="manualOverlay">
    <div class="manual-form">
      <h2>Basic Information</h2>
      
      <div class="form-group">
        <label>Comfortable Languages</label>
        <input type="text" id="manualLanguages" placeholder="e.g., English, Hindi">
      </div>

      <div class="form-group">
        <label>Preferred Color</label>
        <input type="text" id="manualColor" placeholder="e.g., Blue, Green">
      </div>

      <div class="form-group">
        <label>Emergency Contact (Name and Phone)</label>
        <textarea id="manualEmergency" placeholder="e.g., John Doe, +91-1234567890"></textarea>
      </div>

      <div class="form-group">
        <label>Session Goals</label>
        <textarea id="manualGoals" placeholder="What would you like to work on today?"></textarea>
      </div>

      <div class="button-group">
        <button class="btn-secondary" onclick="closeManualInput()">Back</button>
        <button class="btn-primary" onclick="submitManualData()">Start VR</button>
      </div>
    </div>
  </div>

  <!-- Voice Listening Indicator -->
  <div id="voiceIndicator">üé§ Listening...</div>

  <!-- Breathing Circle -->
  <div id="breathingText"></div>
  <div id="breathingCircle"></div>

  <!-- Mobile Touch Controls -->
  <div id="mobileControls" class="mobile-controls">
    <button id="mobileSessionBtn" class="mobile-btn" style="display: none;">Begin Guided Session</button>
    <button id="mobileThumbsUp" class="mobile-btn thumbs-up" style="display: none;">üëç Yes</button>
    <button id="mobileThumbsDown" class="mobile-btn thumbs-down" style="display: none;">üëé Not Really</button>
    <button id="mobileContinue" class="mobile-btn" style="display: none;">Continue to Phase 2</button>
    <button id="mobilePhase3" class="mobile-btn" style="display: none;">Continue to Phase 3</button>
  </div>

  <a-scene>
    <a-sky color="#E8F4F8"></a-sky>
    <a-entity light="type: ambient; color: #BBB; intensity: 0.8"></a-entity>
    <a-entity light="type: directional; color: #FFF; intensity: 0.6" position="5 10 7"></a-entity>
    <a-entity light="type: point; color: #FFF; intensity: 0.4" position="0 3.5 0"></a-entity>

    <a-entity 
      gltf-model="https://raw.githubusercontent.com/o-meiuqer-o/classroom_VR4ET/main/assets/models/classroom.glb"
      position="0 0 0"
      rotation="0 180 0"
      scale="1.15 1.15 1.15"
      shadow="cast: true; receive: true">
    </a-entity>

    <a-entity id="rig" position="-0.5 0.6 1.8">
      <a-camera look-controls wasd-controls>
        <a-cursor color="#21808D" raycaster="objects: .clickable"></a-cursor>
      </a-camera>
    </a-entity>

    <!-- Session Start Button -->
    <a-entity id="sessionButton" 
              position="0 1.6 -2"
              visible="false"
              class="clickable">
      <a-plane width="1.5" height="0.5" color="#21808D" opacity="0.95"></a-plane>
      <a-text value="Begin Guided Session" 
              position="0 0 0.01" 
              align="center"
              width="2.5"
              color="#FFFFFF"
              font="roboto"
              wrap-count="30"></a-text>
    </a-entity>

    <!-- Feedback Buttons (hidden initially) -->
    <a-entity id="feedbackButtons" visible="false">
      <a-entity id="thumbsUpBtn" position="-0.5 1.5 -1.8" class="clickable">
        <a-plane width="0.6" height="0.4" color="#51CF66"></a-plane>
        <a-text value="üëç Yes" position="0 0 0.01" align="center" width="1.2" color="#FFF"></a-text>
      </a-entity>
      
      <a-entity id="thumbsDownBtn" position="0.5 1.5 -1.8" class="clickable">
        <a-plane width="0.6" height="0.4" color="#FF6B6B"></a-plane>
        <a-text value="üëé Not Really" position="0 0 0.01" align="center" width="1.2" color="#FFF"></a-text>
      </a-entity>
    </a-entity>

    <!-- Continue Button (hidden initially) -->
    <a-entity id="continueButton" 
              position="0 1.6 -2"
              visible="false"
              class="clickable">
      <a-plane width="1.8" height="0.5" color="#21808D" opacity="0.95"></a-plane>
      <a-text value="Continue to Phase 2" 
              position="0 0 0.01" 
              align="center"
              width="2.8"
              color="#FFFFFF"
              font="roboto"
              wrap-count="30"></a-text>
    </a-entity>

    <!-- Phase 3 Button (hidden initially) -->
    <a-entity id="phase3Button" 
              position="0 1.6 -2"
              visible="false"
              class="clickable">
      <a-plane width="1.8" height="0.5" color="#21808D" opacity="0.95"></a-plane>
      <a-text value="Continue to Phase 3" 
              position="0 0 0.01" 
              align="center"
              width="2.8"
              color="#FFFFFF"
              font="roboto"
              wrap-count="30"></a-text>
    </a-entity>

    <!-- Text Display (for showing numbers 5-4-3-2-1) -->
    <a-text id="displayText" 
            position="0 2 -1.8" 
            align="center" 
            width="3"
            color="#21808D"
            visible="false"></a-text>
  </a-scene>

  <script>
    console.log('VR Classroom v35 - Phase 2 Conclusion + Phase 3 Prompt - Initialized');

    const sessionData = {
      comfortableLanguages: '',
      preferredColor: '',
      emergencyContact: '',
      goals: '',
      startTime: new Date()
    };

    let recognition = null;
    let synthesis = window.speechSynthesis;
    let voices = [];
    let recognitionTimeout = null;
    let isSpeaking = false;
    let isListeningForResponse = false;
    let responseCallback = null;

    // Grounding exercise state
    let groundingStep = 0;
    const groundingSteps = [
      { text: "We're starting with a grounding exercise. This isn't about relaxation yet‚Äîthis is about bringing you fully into the present moment so your brain knows where you are and what's real.", duration: 6000 },
      { text: "It's called 5-4-3-2-1. We're going to use your senses. I'll guide you.", duration: 4000, showNumbers: true },
      { text: "Look around. FIVE things you can SEE right now. Name them out loud or in your head.", duration: 8000 },
      { text: "FOUR things you can TOUCH or feel right now. Actually reach out and touch them if you can. The chair under you? Your shirt? The floor under your feet? Your hair?", duration: 8000 },
      { text: "THREE things you can HEAR right now. Could be my voice, background sounds, maybe your own breathing.", duration: 8000 },
      { text: "TWO things you can SMELL. If you can't smell anything obvious, that's fine‚Äîjust notice that.", duration: 5000 },
      { text: "ONE thing you can TASTE. Maybe you brushed your teeth earlier, or had water. Or maybe you don't taste anything‚Äîthat's fine too.", duration: 5000 },
      { text: "Good. Now take one deep breath in... and out.", duration: 4000 },
      { text: "Small check-in - do you feel a little more here? A little more solid? You can say yes or no, or tap a button.", duration: 3000, showFeedback: true },
      { text: "Good. That grounding technique! You can use that in the exam hall if you start feeling spaced out. We'll come back to that.", duration: 5000, positive: true },
      { text: "That's okay. Sometimes it takes a few tries. We can do it again later if needed.", duration: 4000, negative: true }
    ];

    // Phase 2: Breathing exercises - WITH FEEDBACK AND PHASE 3 PROMPT
    let breathingStep = 0;
    const breathingSteps = [
      { text: "Phase 2. Now we're going to regulate your nervous system with breathing. It's not just about calming down‚Äîit's about giving your body the signal that you're safe.", duration: 5000 },
      { text: "We'll do box breathing. In for four, hold for four, out for six, hold for two. I'll guide each one.", duration: 5000 },
      // Cycle 1
      { text: "Breathe in", duration: 0, animDuration: 4000, breathe: 'in', breathText: 'BREATHE IN' },
      { text: "Hold", duration: 0, animDuration: 4000, breathe: 'hold-full', breathText: 'HOLD' },
      { text: "Breathe out", duration: 0, animDuration: 6000, breathe: 'out', breathText: 'BREATHE OUT' },
      { text: "Hold", duration: 0, animDuration: 2000, breathe: 'hold-empty', breathText: 'HOLD' },
      // Cycle 2
      { text: "Again. Breathe in", duration: 0, animDuration: 4000, breathe: 'in', breathText: 'BREATHE IN' },
      { text: "Hold", duration: 0, animDuration: 4000, breathe: 'hold-full', breathText: 'HOLD' },
      { text: "Out", duration: 0, animDuration: 6000, breathe: 'out', breathText: 'BREATHE OUT' },
      { text: "Hold", duration: 0, animDuration: 2000, breathe: 'hold-empty', breathText: 'HOLD' },
      // Cycle 3
      { text: "Last one. In", duration: 0, animDuration: 4000, breathe: 'in', breathText: 'BREATHE IN' },
      { text: "Hold", duration: 0, animDuration: 4000, breathe: 'hold-full', breathText: 'HOLD' },
      { text: "Out", duration: 0, animDuration: 6000, breathe: 'out', breathText: 'BREATHE OUT' },
      // Phase 2 Reflection
      { text: "Good. Notice your shoulders, your jaw. Are they a little looser? Did your heart rate drop even a bit? You don't have to feel perfect. You just have to feel present.", duration: 6000 },
      // Phase 2 Feedback Question
      { text: "How do you feel after that breathing exercise? Ready to continue, or do you need a moment? You can say yes or no, or tap a button.", duration: 4000, showFeedback: true },
      // Phase 2 Conclusion (after positive feedback)
      { text: "Great. So. Phase 1 was grounding. That gets you here. Phase 2 was breathing. That tells your body you're safe. Together, these are your reset button.", duration: 6000, conclusion: true, positive: true },
      { text: "You can do these anywhere. In the exam hall. Before bed. In a bathroom stall if you need to. No one even has to know.", duration: 5000, conclusion: true, positive: true },
      // Phase 2 Alternative conclusion (after negative feedback)
      { text: "That's okay. These techniques take practice. The more you use them, the more effective they become.", duration: 5000, conclusion: true, negative: true },
      { text: "Even if they don't feel perfect right now, your nervous system is still learning. That's progress.", duration: 4000, conclusion: true, negative: true }
    ];

    if (synthesis) {
      synthesis.onvoiceschanged = () => {
        voices = synthesis.getVoices();
        console.log('Voices loaded:', voices.length);
      };
    }

    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      recognition = new SpeechRecognition();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'en-US';
      console.log('Speech Recognition initialized');
    }

    let conversationState = {
      step: 'welcome',
      responses: {},
      isListening: false
    };

    const conversationFlow = {
      welcome: {
        text: "Welcome to Virtual Lecter's Destressing Experience. We'll practice safely in a virtual classroom. Before we begin, I will ask a few questions. Are you ready to start?",
        nextStep: 'language'
      },
      language: {
        text: "Which languages are you comfortable with? Feel free to mention one or more.",
        nextStep: 'color',
        saveAs: 'comfortableLanguages'
      },
      color: {
        text: "What is your preferred color for the virtual environment?",
        nextStep: 'emergencyContact',
        saveAs: 'preferredColor'
      },
      emergencyContact: {
        text: "For your safety, please provide the name and phone number of an emergency contact person.",
        nextStep: 'goals',
        saveAs: 'emergencyContact'
      },
      goals: {
        text: "What would you like to work on today?",
        nextStep: 'complete',
        saveAs: 'goals'
      },
      complete: {
        text: "Thank you. We're ready to begin. Are you ready to enter the virtual environment?",
        nextStep: 'vr'
      }
    };

    function speak(text, callback) {
      console.log('Speaking:', text.substring(0, 40) + '...');
      
      if (!synthesis) {
        console.log('Speech synthesis not available');
        if (callback) setTimeout(callback, 500);
        return;
      }

      synthesis.cancel();
      isSpeaking = true;

      const utterance = new SpeechSynthesisUtterance(text);
      utterance.rate = 0.9;
      utterance.pitch = 1.0;
      utterance.lang = 'en-US';

      if (voices.length > 0) {
        const voice = voices.find(v => v.lang.includes('en') && v.name.includes('Female')) || voices[0];
        utterance.voice = voice;
      }

      let completed = false;
      const finishSpeech = () => {
        if (!completed) {
          completed = true;
          isSpeaking = false;
          console.log('Speech completed');
          if (callback) {
            setTimeout(callback, 200);
          }
        }
      };

      utterance.onend = finishSpeech;
      utterance.onerror = (e) => {
        console.error('Speech error:', e);
        finishSpeech();
      };

      const wordCount = text.split(' ').length;
      const estimatedDuration = (wordCount * 140) + 1000;
      setTimeout(finishSpeech, estimatedDuration);

      addMessage('System', text);
      synthesis.speak(utterance);
    }

    function addMessage(sender, text) {
      const display = document.getElementById('conversationDisplay');
      const msg = document.createElement('div');
      msg.className = `message ${sender.toLowerCase()}`;
      msg.innerHTML = `<strong>${sender}:</strong> ${text}`;
      display.appendChild(msg);
      display.scrollTop = display.scrollHeight;
    }

    function addOrUpdateLiveCaption(text) {
      const display = document.getElementById('conversationDisplay');
      let caption = document.getElementById('live-caption');
      
      if (!caption) {
        caption = document.createElement('div');
        caption.id = 'live-caption';
        caption.className = 'message user live';
        display.appendChild(caption);
      }
      
      caption.innerHTML = `<strong>You (speaking):</strong> ${text}`;
      display.scrollTop = display.scrollHeight;
    }

    function removeLiveCaption() {
      const caption = document.getElementById('live-caption');
      if (caption) caption.remove();
    }

    async function requestMicPermission() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        stream.getTracks().forEach(track => track.stop());
        console.log('Microphone permission granted');
        return true;
      } catch (error) {
        console.error('Microphone permission denied:', error);
        alert('Please allow microphone access.');
        return false;
      }
    }

    // Voice response listener for yes/no questions
    function startVoiceResponseListener(callback) {
      if (!recognition) {
        console.log('Voice recognition not available');
        return;
      }

      console.log('Starting voice response listener, callback:', typeof callback);
      isListeningForResponse = true;
      responseCallback = callback;

      document.getElementById('voiceIndicator').classList.add('active');

      if (recognitionTimeout) clearTimeout(recognitionTimeout);
      recognitionTimeout = setTimeout(() => {
        console.log('Voice response timeout');
        stopVoiceResponseListener();
      }, 45000);

      recognition.onstart = () => {
        console.log('Voice recognition started for response');
      };

      recognition.onresult = (event) => {
        let interimTranscript = '';
        let finalTranscript = '';
        
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript;
          } else {
            interimTranscript += transcript;
          }
        }

        if (interimTranscript) {
          addOrUpdateLiveCaption(interimTranscript);
        }

        if (finalTranscript) {
          const transcript = finalTranscript.trim().toLowerCase();
          console.log('Voice response detected:', transcript);
          removeLiveCaption();

          const isYes = transcript.includes('yes') || transcript.includes('yeah') || 
                        transcript.includes('yep') || transcript.includes('sure') ||
                        transcript.includes('okay') || transcript.includes('ok') ||
                        transcript.includes('fine');
          
          const isNo = transcript.includes('no') || transcript.includes('nope') || 
                       transcript.includes('not really') || transcript.includes('nah') ||
                       transcript.includes('not yet');

          const isContinue = transcript.includes('continue') || transcript.includes('next') ||
                             transcript.includes('proceed') || transcript.includes('go ahead') ||
                             transcript.includes('let\'s go');

          console.log('Detected - Yes:', isYes, 'No:', isNo, 'Continue:', isContinue);

          if (isYes || isNo || isContinue) {
            console.log('Response recognized, calling callback with:', (isContinue || isYes));
            
            if (recognitionTimeout) clearTimeout(recognitionTimeout);
            stopVoiceResponseListener();
            
            if (responseCallback && typeof responseCallback === 'function') {
              const result = (isContinue || isYes);
              console.log('Executing callback with result:', result);
              responseCallback(result);
            } else {
              console.error('No valid callback found!');
            }
          }
        }
      };

      recognition.onerror = (event) => {
        console.error('Voice response error:', event.error);
        removeLiveCaption();
        
        if (event.error !== 'no-speech' && event.error !== 'aborted') {
          stopVoiceResponseListener();
        }
      };

      recognition.onend = () => {
        console.log('Recognition ended, isListening:', isListeningForResponse);
        if (isListeningForResponse) {
          setTimeout(() => {
            try {
              console.log('Restarting recognition');
              recognition.start();
            } catch (e) {
              console.error('Voice response restart failed:', e);
            }
          }, 100);
        }
      };

      try {
        recognition.start();
      } catch (error) {
        console.error('Failed to start voice response:', error);
      }
    }

    function stopVoiceResponseListener() {
      console.log('Stopping voice response listener');
      isListeningForResponse = false;
      responseCallback = null;
      document.getElementById('voiceIndicator').classList.remove('active');
      removeLiveCaption();
      
      if (recognition) {
        try {
          recognition.stop();
        } catch (e) {
          console.log('Recognition already stopped');
        }
      }
    }

    async function startConversation() {
      const voiceBtn = document.getElementById('voiceBtn');
      const statusEl = document.getElementById('voiceStatus');

      const currentStep = conversationState.step;
      const currentFlow = conversationFlow[currentStep];

      if (currentStep !== 'welcome' && Object.keys(conversationState.responses).length > 0) {
        console.log('Resuming conversation');
        voiceBtn.textContent = 'üîä Resuming...';
        speak("Let's continue. " + currentFlow.text, () => {
          startListening(currentStep);
        });
        return;
      }

      voiceBtn.textContent = 'üé§ Requesting microphone...';
      const hasPermission = await requestMicPermission();
      
      if (!hasPermission) {
        voiceBtn.textContent = 'üé§ Click to Start';
        statusEl.textContent = '‚ùå Microphone denied';
        return;
      }

      voiceBtn.textContent = 'üîä Speaking...';
      speak(conversationFlow.welcome.text, () => {
        startListening('welcome');
      });
    }

    function startListening(step) {
      if (!recognition) {
        alert('Voice recognition not supported. Use manual input.');
        return;
      }

      console.log('Starting to listen for:', step);
      
      const voiceBtn = document.getElementById('voiceBtn');
      const statusEl = document.getElementById('voiceStatus');

      conversationState.step = step;
      conversationState.isListening = true;

      voiceBtn.textContent = 'üé§ Listening...';
      voiceBtn.classList.add('listening');
      statusEl.textContent = 'üî¥ LISTENING';
      statusEl.style.color = '#FF6B6B';
      statusEl.style.fontWeight = 'bold';

      if (recognitionTimeout) clearTimeout(recognitionTimeout);

      recognitionTimeout = setTimeout(() => {
        console.log('Recognition timeout');
        recognition.stop();
      }, 60000);

      recognition.onstart = () => {
        console.log('Recognition started');
      };

      recognition.onresult = (event) => {
        let interimTranscript = '';
        let finalTranscript = '';
        
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript;
          } else {
            interimTranscript += transcript;
          }
        }

        if (interimTranscript) {
          statusEl.textContent = `üìù "${interimTranscript}"`;
          addOrUpdateLiveCaption(interimTranscript);
        }

        if (finalTranscript) {
          const transcript = finalTranscript.trim();
          console.log('Final result:', transcript);
          removeLiveCaption();

          if (recognitionTimeout) clearTimeout(recognitionTimeout);
          conversationState.isListening = false;

          voiceBtn.classList.remove('listening');
          voiceBtn.textContent = '‚úÖ Got it!';
          statusEl.textContent = `You said: "${transcript}"`;
          statusEl.style.color = '#51CF66';

          addMessage('You', transcript);
          processResponse(transcript, step);
        }
      };

      recognition.onerror = (event) => {
        console.error('Recognition error:', event.error);
        removeLiveCaption();
        
        if (event.error !== 'no-speech') {
          voiceBtn.classList.remove('listening');
          voiceBtn.textContent = 'üîÑ Click to Resume';
          statusEl.textContent = 'Error: ' + event.error;
          conversationState.isListening = false;
        }
      };

      recognition.onend = () => {
        console.log('Recognition ended');
        if (conversationState.isListening) {
          setTimeout(() => {
            try {
              recognition.start();
            } catch (e) {
              console.error('Restart failed:', e);
            }
          }, 100);
        }
      };

      try {
        recognition.start();
      } catch (error) {
        console.error('Failed to start:', error);
      }
    }

    function processResponse(transcript, step) {
      const currentFlow = conversationFlow[step];
      
      if (currentFlow.saveAs) {
        conversationState.responses[currentFlow.saveAs] = transcript;
      }

      if (currentFlow.nextStep === 'vr') {
        completeConversation();
      } else {
        setTimeout(() => continueConversation(currentFlow.nextStep), 1000);
      }
    }

    function continueConversation(nextStep) {
      const nextFlow = conversationFlow[nextStep];
      const voiceBtn = document.getElementById('voiceBtn');
      const statusEl = document.getElementById('voiceStatus');

      voiceBtn.textContent = 'üîä Speaking...';
      voiceBtn.classList.remove('listening');
      statusEl.textContent = 'Speaking';
      statusEl.style.color = '';
      statusEl.style.fontWeight = '';

      speak(nextFlow.text, () => {
        startListening(nextStep);
      });
    }

    function completeConversation() {
      Object.assign(sessionData, conversationState.responses);
      console.log('Conversation complete:', sessionData);

      const voiceBtn = document.getElementById('voiceBtn');
      voiceBtn.textContent = '‚úÖ Complete';

      setTimeout(() => {
        document.getElementById('vrOverlay').classList.add('hidden');
        document.getElementById('sessionButton').setAttribute('visible', 'true');
        showMobileSessionButton();
        speak('Welcome to the virtual classroom. When ready, tap or click the Begin Guided Session button.');
      }, 2000);
    }

    function showManualInput() {
      document.getElementById('manualOverlay').classList.add('active');
    }

    function closeManualInput() {
      document.getElementById('manualOverlay').classList.remove('active');
    }

    function submitManualData() {
      sessionData.comfortableLanguages = document.getElementById('manualLanguages').value;
      sessionData.preferredColor = document.getElementById('manualColor').value;
      sessionData.emergencyContact = document.getElementById('manualEmergency').value;
      sessionData.goals = document.getElementById('manualGoals').value;

      console.log('Manual data submitted:', sessionData);
      closeManualInput();
      document.getElementById('vrOverlay').classList.add('hidden');
      document.getElementById('sessionButton').setAttribute('visible', 'true');
      showMobileSessionButton();
      speak('Welcome to the virtual classroom. When ready, tap or click the Begin Guided Session button.');
    }

    // Mobile Control Functions
    function showMobileSessionButton() {
      document.getElementById('mobileControls').classList.add('active');
      document.getElementById('mobileSessionBtn').style.display = 'block';
    }

    function hideMobileSessionButton() {
      document.getElementById('mobileSessionBtn').style.display = 'none';
    }

    function showMobileFeedbackButtons() {
      document.getElementById('mobileControls').classList.add('active');
      document.getElementById('mobileThumbsUp').style.display = 'block';
      document.getElementById('mobileThumbsDown').style.display = 'block';
    }

    function hideMobileFeedbackButtons() {
      document.getElementById('mobileThumbsUp').style.display = 'none';
      document.getElementById('mobileThumbsDown').style.display = 'none';
    }

    function showMobileContinueButton() {
      document.getElementById('mobileControls').classList.add('active');
      document.getElementById('mobileContinue').style.display = 'block';
    }

    function hideMobileContinueButton() {
      document.getElementById('mobileContinue').style.display = 'none';
    }

    function showMobilePhase3Button() {
      document.getElementById('mobileControls').classList.add('active');
      document.getElementById('mobilePhase3').style.display = 'block';
    }

    function hideMobilePhase3Button() {
      document.getElementById('mobilePhase3').style.display = 'none';
    }

    // Grounding Exercise Functions
    function startGuidedSession() {
      console.log('Starting guided session - Phase 1: Grounding');
      groundingStep = 0;
      document.getElementById('sessionButton').setAttribute('visible', 'false');
      hideMobileSessionButton();
      nextGroundingStep();
    }

    function nextGroundingStep() {
      if (groundingStep >= groundingSteps.length - 2) return;

      const step = groundingSteps[groundingStep];
      console.log('Grounding step', groundingStep, ':', step.text.substring(0, 40));

      if (step.showNumbers) {
        document.getElementById('displayText').setAttribute('value', '5  4  3  2  1');
        document.getElementById('displayText').setAttribute('visible', 'true');
        setTimeout(() => {
          document.getElementById('displayText').setAttribute('visible', 'false');
        }, 3000);
      }

      if (step.showFeedback) {
        speak(step.text, () => {
          document.getElementById('feedbackButtons').setAttribute('visible', 'true');
          showMobileFeedbackButtons();
          
          console.log('Setting up voice listener for grounding feedback');
          startVoiceResponseListener((response) => {
            console.log('Grounding feedback callback received:', response);
            handleFeedback(response);
          });
        });
        return;
      }

      speak(step.text, () => {
        setTimeout(() => {
          groundingStep++;
          nextGroundingStep();
        }, step.duration);
      });
    }

    function handleFeedback(isPositive) {
      console.log('handleFeedback called with:', isPositive);
      stopVoiceResponseListener();
      document.getElementById('feedbackButtons').setAttribute('visible', 'false');
      hideMobileFeedbackButtons();
      
      const responseStep = isPositive ? 9 : 10;
      speak(groundingSteps[responseStep].text, () => {
        console.log('Grounding exercise complete');
        askToContinuePhase2();
      });
    }

    function askToContinuePhase2() {
      speak('Would you like to continue to Phase 2: Breathing exercises? You can say yes, or tap the button.', () => {
        document.getElementById('continueButton').setAttribute('visible', 'true');
        showMobileContinueButton();
        
        console.log('Setting up voice listener for phase 2 continuation');
        startVoiceResponseListener((response) => {
          console.log('Phase 2 continuation callback received:', response);
          if (response) {
            startPhase2();
          } else {
            stopVoiceResponseListener();
            document.getElementById('continueButton').setAttribute('visible', 'false');
            hideMobileContinueButton();
            speak('That\'s okay. Take your time. You can restart whenever you\'re ready.');
          }
        });
      });
    }

    function startPhase2() {
      console.log('Starting Phase 2');
      stopVoiceResponseListener();
      document.getElementById('continueButton').setAttribute('visible', 'false');
      hideMobileContinueButton();
      
      breathingStep = 0;
      nextBreathingStep();
    }

    function nextBreathingStep() {
      if (breathingStep >= breathingSteps.length) {
        console.log('Phase 2 complete');
        document.getElementById('breathingCircle').classList.remove('active');
        document.getElementById('breathingText').classList.remove('active');
        return;
      }

      const step = breathingSteps[breathingStep];
      console.log('Breathing step', breathingStep, ':', step.text.substring(0, 40));

      const circle = document.getElementById('breathingCircle');
      const breathText = document.getElementById('breathingText');
      
      // Update breathing visualization
      if (step.breathe) {
        circle.classList.remove('inhale', 'exhale', 'hold-full', 'hold-empty');
        circle.classList.add('active', step.breathe);
        breathText.textContent = step.breathText || '';
        breathText.classList.add('active');
      } else {
        circle.classList.remove('active', 'inhale', 'exhale', 'hold-full', 'hold-empty');
        breathText.classList.remove('active');
      }

      // Handle feedback steps in Phase 2
      if (step.showFeedback) {
        speak(step.text, () => {
          document.getElementById('feedbackButtons').setAttribute('visible', 'true');
          showMobileFeedbackButtons();
          
          console.log('Setting up voice listener for Phase 2 feedback');
          startVoiceResponseListener((response) => {
            console.log('Phase 2 feedback callback received:', response);
            handlePhase2Feedback(response);
          });
        });
        return;
      }

      // Handle conclusion steps
      if (step.conclusion) {
        speak(step.text, () => {
          setTimeout(() => {
            breathingStep++;
            nextBreathingStep();
          }, step.duration || 3000);
        });
        return;
      }

      // For breathing steps: speak immediately, then wait for animation
      if (step.animDuration) {
        speak(step.text, () => {
          // After speech completes (200ms delay), wait for animation
          setTimeout(() => {
            breathingStep++;
            nextBreathingStep();
          }, step.animDuration);
        });
      } else {
        // Non-breathing steps
        speak(step.text, () => {
          setTimeout(() => {
            breathingStep++;
            nextBreathingStep();
          }, step.duration);
        });
      }
    }

    function handlePhase2Feedback(isPositive) {
      console.log('handlePhase2Feedback called with:', isPositive);
      stopVoiceResponseListener();
      document.getElementById('feedbackButtons').setAttribute('visible', 'false');
      hideMobileFeedbackButtons();
      document.getElementById('breathingCircle').classList.remove('active');
      document.getElementById('breathingText').classList.remove('active');
      
      // Move to conclusion steps based on feedback
      if (isPositive) {
        breathingStep = 15; // Start with positive conclusion
      } else {
        breathingStep = 17; // Start with negative conclusion
      }
      
      nextBreathingStep();
      
      // After conclusion, ask about Phase 3
      setTimeout(() => {
        askToContinuePhase3();
      }, 10000); // Wait for conclusion steps to finish
    }

    function askToContinuePhase3() {
      speak('Would you like to continue to Phase 3? You can say yes, or tap the button.', () => {
        document.getElementById('phase3Button').setAttribute('visible', 'true');
        showMobilePhase3Button();
        
        console.log('Setting up voice listener for phase 3 continuation');
        startVoiceResponseListener((response) => {
          console.log('Phase 3 continuation callback received:', response);
          if (response) {
            startPhase3();
          } else {
            stopVoiceResponseListener();
            document.getElementById('phase3Button').setAttribute('visible', 'false');
            hideMobilePhase3Button();
            speak('That\'s perfectly fine. You\'ve accomplished a lot today. These two phases alone are powerful tools for managing stress and anxiety.');
          }
        });
      });
    }

    function startPhase3() {
      console.log('Starting Phase 3');
      stopVoiceResponseListener();
      document.getElementById('phase3Button').setAttribute('visible', 'false');
      hideMobilePhase3Button();
      
      speak('Phase 3 will be implemented in the next update. For now, you can explore the virtual classroom and practice what you\'ve learned.');
    }

    // Event Listeners
    document.getElementById('voiceBtn').addEventListener('click', startConversation);
    document.getElementById('skipBtn').addEventListener('click', showManualInput);

    document.getElementById('mobileSessionBtn').addEventListener('click', startGuidedSession);
    document.getElementById('mobileSessionBtn').addEventListener('touchend', (e) => {
      e.preventDefault();
      startGuidedSession();
    });

    document.getElementById('mobileThumbsUp').addEventListener('click', () => {
      console.log('Thumbs up button clicked');
      if (groundingStep > 0 && breathingStep === 0) {
        handleFeedback(true);
      } else if (breathingStep > 0) {
        handlePhase2Feedback(true);
      }
    });
    document.getElementById('mobileThumbsUp').addEventListener('touchend', (e) => {
      e.preventDefault();
      console.log('Thumbs up touch');
      if (groundingStep > 0 && breathingStep === 0) {
        handleFeedback(true);
      } else if (breathingStep > 0) {
        handlePhase2Feedback(true);
      }
    });

    document.getElementById('mobileThumbsDown').addEventListener('click', () => {
      console.log('Thumbs down button clicked');
      if (groundingStep > 0 && breathingStep === 0) {
        handleFeedback(false);
      } else if (breathingStep > 0) {
        handlePhase2Feedback(false);
      }
    });
    document.getElementById('mobileThumbsDown').addEventListener('touchend', (e) => {
      e.preventDefault();
      console.log('Thumbs down touch');
      if (groundingStep > 0 && breathingStep === 0) {
        handleFeedback(false);
      } else if (breathingStep > 0) {
        handlePhase2Feedback(false);
      }
    });

    document.getElementById('mobileContinue').addEventListener('click', () => {
      console.log('Continue button clicked');
      startPhase2();
    });
    document.getElementById('mobileContinue').addEventListener('touchend', (e) => {
      e.preventDefault();
      console.log('Continue touch');
      startPhase2();
    });

    document.getElementById('mobilePhase3').addEventListener('click', () => {
      console.log('Phase 3 button clicked');
      startPhase3();
    });
    document.getElementById('mobilePhase3').addEventListener('touchend', (e) => {
      e.preventDefault();
      console.log('Phase 3 touch');
      startPhase3();
    });

    document.addEventListener('DOMContentLoaded', () => {
      const sessionBtn = document.getElementById('sessionButton');
      sessionBtn.addEventListener('click', startGuidedSession);

      document.getElementById('thumbsUpBtn').addEventListener('click', () => {
        if (groundingStep > 0 && breathingStep === 0) {
          handleFeedback(true);
        } else if (breathingStep > 0) {
          handlePhase2Feedback(true);
        }
      });
      document.getElementById('thumbsDownBtn').addEventListener('click', () => {
        if (groundingStep > 0 && breathingStep === 0) {
          handleFeedback(false);
        } else if (breathingStep > 0) {
          handlePhase2Feedback(false);
        }
      });
      
      document.getElementById('continueButton').addEventListener('click', () => startPhase2());
      document.getElementById('phase3Button').addEventListener('click', () => startPhase3());
    });
  </script>
</body>
</html>

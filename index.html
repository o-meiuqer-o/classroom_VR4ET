<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>VR Classroom Exposure Therapy</title>
  <script src="https://aframe.io/releases/1.4.2/aframe.min.js"></script>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
      overflow: hidden;
      background: linear-gradient(135deg, #E8F4F8 0%, #F0F8FA 100%);
    }

    /* VR Overlay */
    #vrOverlay {
      position: fixed;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      backdrop-filter: blur(8px);
      background: rgba(232, 244, 248, 0.85);
      display: flex;
      align-items: center;
      justify-content: center;
      z-index: 1000;
      transition: opacity 0.5s ease, visibility 0.5s ease;
    }

    #vrOverlay.hidden {
      opacity: 0;
      visibility: hidden;
      pointer-events: none;
    }

    .overlay-card {
      max-width: 420px;
      width: 90%;
      background: white;
      border-radius: 16px;
      padding: 24px;
      box-shadow: 0 10px 40px rgba(19, 52, 59, 0.15);
      animation: slideUp 0.4s ease;
    }

    @keyframes slideUp {
      from {
        opacity: 0;
        transform: translateY(20px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    .overlay-card h1 {
      font-size: 20px;
      color: rgba(19, 52, 59, 1);
      margin-bottom: 8px;
      font-weight: 600;
    }

    .overlay-card p {
      font-size: 13px;
      color: rgba(19, 52, 59, 0.7);
      margin-bottom: 20px;
      line-height: 1.5;
    }

    /* Conversation Display */
    #conversationDisplay {
      max-height: 200px;
      overflow-y: auto;
      margin-bottom: 16px;
      padding: 12px;
      background: rgba(232, 244, 248, 0.5);
      border-radius: 8px;
    }

    .message {
      margin-bottom: 10px;
      padding: 8px 12px;
      border-radius: 8px;
      font-size: 12px;
      line-height: 1.4;
    }

    .message.system {
      background: rgba(33, 128, 141, 0.1);
      color: rgba(19, 52, 59, 1);
      border-left: 3px solid rgba(33, 128, 141, 1);
    }

    .message.user {
      background: rgba(33, 128, 141, 0.2);
      color: rgba(19, 52, 59, 1);
      text-align: right;
      border-right: 3px solid rgba(33, 128, 141, 1);
    }

    /* Live Caption */
    #liveCaption {
      min-height: 40px;
      padding: 10px;
      background: rgba(255, 255, 255, 0.8);
      border-radius: 8px;
      margin-bottom: 16px;
      font-size: 12px;
      color: rgba(19, 52, 59, 0.6);
      font-style: italic;
      text-align: center;
    }

    /* Voice Button */
    #voiceButton {
      width: 100%;
      padding: 14px;
      border: none;
      border-radius: 8px;
      background: #FF6B6B;
      color: white;
      font-size: 14px;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.3s ease;
      margin-bottom: 12px;
      display: flex;
      align-items: center;
      justify-content: center;
      gap: 8px;
    }

    #voiceButton:hover {
      transform: translateY(-2px);
      box-shadow: 0 4px 12px rgba(255, 107, 107, 0.3);
    }

    #voiceButton.listening {
      background: #51CF66;
      animation: pulse 1.5s infinite;
    }

    @keyframes pulse {
      0%, 100% {
        opacity: 1;
      }
      50% {
        opacity: 0.7;
      }
    }

    #voiceButton.processing {
      background: #FFA94D;
      cursor: not-allowed;
    }

    #voiceButton:disabled {
      opacity: 0.6;
      cursor: not-allowed;
    }

    .status-icon {
      width: 10px;
      height: 10px;
      border-radius: 50%;
      background: white;
      display: inline-block;
      animation: blink 1s infinite;
    }

    @keyframes blink {
      0%, 100% { opacity: 1; }
      50% { opacity: 0.3; }
    }

    #skipButton {
      width: 100%;
      padding: 10px;
      border: 2px solid rgba(33, 128, 141, 1);
      border-radius: 8px;
      background: transparent;
      color: rgba(33, 128, 141, 1);
      font-size: 13px;
      font-weight: 500;
      cursor: pointer;
      transition: all 0.3s ease;
    }

    #skipButton:hover {
      background: rgba(33, 128, 141, 0.1);
    }

    /* Manual Input Overlay */
    #manualInputOverlay {
      display: none;
    }

    #manualInputOverlay.active {
      display: flex;
    }

    .form-group {
      margin-bottom: 16px;
    }

    .form-group label {
      display: block;
      font-size: 13px;
      font-weight: 500;
      color: rgba(19, 52, 59, 1);
      margin-bottom: 6px;
    }

    .form-group input,
    .form-group textarea {
      width: 100%;
      padding: 10px 12px;
      border: 1px solid rgba(19, 52, 59, 0.2);
      border-radius: 6px;
      font-size: 13px;
      font-family: inherit;
      transition: border-color 0.3s ease;
    }

    .form-group input:focus,
    .form-group textarea:focus {
      outline: none;
      border-color: rgba(33, 128, 141, 1);
    }

    .form-group textarea {
      resize: vertical;
      min-height: 60px;
    }

    .button-group {
      display: flex;
      gap: 10px;
      margin-top: 20px;
    }

    .button-group button {
      flex: 1;
      padding: 12px;
      border: none;
      border-radius: 8px;
      font-size: 13px;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.3s ease;
    }

    .button-group button.primary {
      background: rgba(33, 128, 141, 1);
      color: white;
    }

    .button-group button.primary:hover {
      background: rgba(33, 128, 141, 0.9);
    }

    .button-group button.secondary {
      background: rgba(19, 52, 59, 0.1);
      color: rgba(19, 52, 59, 1);
    }

    .button-group button.secondary:hover {
      background: rgba(19, 52, 59, 0.15);
    }

    /* Permission Instructions */
    #permissionInstructions {
      display: none;
      margin-top: 12px;
      padding: 12px;
      background: rgba(255, 193, 7, 0.1);
      border-left: 3px solid #FFC107;
      border-radius: 4px;
      font-size: 11px;
      color: rgba(19, 52, 59, 0.8);
      line-height: 1.5;
    }

    #permissionInstructions.show {
      display: block;
    }

    /* Status Message */
    #statusMessage {
      text-align: center;
      padding: 8px;
      font-size: 11px;
      color: rgba(19, 52, 59, 0.6);
      font-weight: 500;
    }
  </style>
</head>
<body>
  <!-- VR Scene -->
  <a-scene>
    <!-- Sky -->
    <a-sky color="#E8F4F8"></a-sky>

    <!-- Lighting -->
    <a-entity light="type: ambient; color: #BBB; intensity: 0.8"></a-entity>
    <a-entity light="type: directional; intensity: 0.6" position="5 10 7"></a-entity>
    <a-entity light="type: point; intensity: 0.4" position="0 3.5 0"></a-entity>

    <!-- Camera Rig -->
    <a-entity id="cameraRig" position="-0.5 0.6 1.8">
      <a-camera wasd-controls="acceleration: 20" look-controls="pointerLockEnabled: true">
        <a-cursor></a-cursor>
      </a-camera>
    </a-entity>

    <!-- Classroom Model -->
    <a-entity 
      id="classroom"
      gltf-model="https://raw.githubusercontent.com/o-meiuqer-o/classroom_VR4ET/main/assets/models/classroom.glb"
      position="0 0 0"
      rotation="0 180 0"
      scale="1.15 1.15 1.15">
    </a-entity>

    <!-- VR UI Elements (hidden initially) -->
    <!-- Begin Guided Session Button -->
    <a-entity id="beginSessionBtn" visible="false" position="0 1.6 -2">
      <a-plane width="1.2" height="0.4" color="#21808D" opacity="0.95" class="clickable"></a-plane>
      <a-text value="Begin Guided Session" align="center" position="0 0 0.01" width="2" color="#FCFCF9"></a-text>
    </a-entity>

    <!-- Dialogue Panel -->
    <a-entity id="dialoguePanel" visible="false" position="0 1.6 -1.5">
      <!-- Background -->
      <a-plane width="2.5" height="1.4" color="#FCFCF9" opacity="0.95" class="panel-bg">
        <a-plane width="2.4" height="1.3" position="0 0 0.01" color="#E8F4F8" opacity="0.5"></a-plane>
      </a-plane>
      <!-- Progress Indicator -->
      <a-text id="progressText" value="Phase 1 of 6: Grounding" align="center" position="0 0.6 0.02" width="2.2" color="#134252" font="roboto"></a-text>
      <!-- Main Text -->
      <a-text id="dialogueText" value="" align="center" position="0 0.1 0.02" width="2.2" color="#134252" wrap-count="40" baseline="center"></a-text>
      <!-- Visual Text (for numbers like 5-4-3-2-1) -->
      <a-text id="visualText" value="" align="center" position="0 -0.2 0.02" width="3" color="#21808D" wrap-count="40" visible="false"></a-text>
    </a-entity>

    <!-- Feedback Buttons -->
    <a-entity id="feedbackButtons" visible="false">
      <!-- Yes Button -->
      <a-entity id="yesBtn" position="-0.5 1.3 -1.5" class="clickable">
        <a-plane width="0.7" height="0.35" color="#51CF66" opacity="0.95"></a-plane>
        <a-text value="👍 Yes" align="center" position="0 0 0.01" width="1.5" color="#FCFCF9"></a-text>
      </a-entity>
      <!-- No Button -->
      <a-entity id="noBtn" position="0.5 1.3 -1.5" class="clickable">
        <a-plane width="0.7" height="0.35" color="#FF6B6B" opacity="0.95"></a-plane>
        <a-text value="👎 Not Really" align="center" position="0 0 0.01" width="1.5" color="#FCFCF9"></a-text>
      </a-entity>
    </a-entity>

    <!-- Continue Button -->
    <a-entity id="continueBtn" visible="false" position="0 1.3 -1.5" class="clickable">
      <a-plane width="0.8" height="0.35" color="#21808D" opacity="0.95"></a-plane>
      <a-text value="Continue" align="center" position="0 0 0.01" width="1.8" color="#FCFCF9"></a-text>
    </a-entity>
  </a-scene>

  <!-- Voice Mode Overlay -->
  <div id="vrOverlay">
    <div class="overlay-card">
      <h1>Virtual Lecter's Experience</h1>
      <p>Welcome to your safe virtual classroom for destressing practice.</p>

      <div id="conversationDisplay"></div>
      <div id="liveCaption">Your responses will appear here...</div>

      <button id="voiceButton">
        <span class="status-icon"></span>
        <span id="voiceButtonText">Start Voice Conversation</span>
      </button>

      <button id="skipButton">Use Manual Input Instead</button>

      <div id="permissionInstructions">
        <strong>Microphone Access Required:</strong><br>
        • Chrome/Edge: Click the camera icon in the address bar<br>
        • Firefox: Click the microphone icon in the address bar<br>
        • Safari: Go to Settings → Websites → Microphone<br>
        Please allow microphone access and try again.
      </div>

      <div id="statusMessage"></div>
    </div>
  </div>

  <!-- Manual Input Overlay -->
  <div id="manualInputOverlay" class="vrOverlay">
    <div class="overlay-card">
      <h1>Session Information</h1>
      <p>Please provide some information to personalize your experience.</p>

      <form id="manualForm">
        <div class="form-group">
          <label>Comfortable Languages</label>
          <input type="text" id="manualLanguages" placeholder="e.g., English, Spanish" required>
        </div>

        <div class="form-group">
          <label>Preferred Color for Environment</label>
          <input type="text" id="manualColor" placeholder="e.g., blue, green, warm" required>
        </div>

        <div class="form-group">
          <label>Emergency Contact (Name &amp; Phone)</label>
          <input type="text" id="manualContact" placeholder="e.g., John Smith 555-1234" required>
        </div>

        <div class="form-group">
          <label>Session Goals</label>
          <textarea id="manualGoals" placeholder="What would you like to work on today?" required></textarea>
        </div>

        <div class="button-group">
          <button type="button" class="secondary" id="backButton">Back</button>
          <button type="submit" class="primary">Begin VR Session</button>
        </div>
      </form>
    </div>
  </div>

  <script>
    // Session State Management
    const sessionState = {
      active: false,
      currentPhase: 'grounding',
      currentStep: 0,
      responses: {
        groundingFeedback: null
      },
      startTime: null,
      phaseStartTime: null
    };

    // Grounding Phase Data
    const groundingPhase = {
      steps: [
        {
          id: 'intro',
          text: "We're starting with a grounding exercise. This isn't about relaxation yet—this is about bringing you fully into the present moment so your brain knows where you are and what's real.",
          duration: 5000
        },
        {
          id: 'technique_name',
          text: "It's called 5-4-3-2-1. We're going to use your senses. I'll guide you.",
          duration: 3000,
          visual: '5-4-3-2-1'
        },
        {
          id: 'five_see',
          text: 'Look around. FIVE things you can SEE right now. Name them out loud or in your head.',
          duration: 8000
        },
        {
          id: 'four_touch',
          text: 'FOUR things you can TOUCH or feel right now. Actually reach out and touch them if you can.',
          prompt: 'The chair under you? Your shirt? The floor under your feet? Your hair?',
          duration: 8000
        },
        {
          id: 'three_hear',
          text: 'THREE things you can HEAR right now. Could be my voice, background sounds, maybe your own breathing.',
          duration: 8000
        },
        {
          id: 'two_smell',
          text: "TWO things you can SMELL. If you can't smell anything obvious, that's fine—just notice that.",
          duration: 5000
        },
        {
          id: 'one_taste',
          text: "ONE thing you can TASTE. Maybe you brushed your teeth earlier, or had water. Or maybe you don't taste anything—that's fine too.",
          duration: 5000
        },
        {
          id: 'breathing',
          text: 'Good. Now take one deep breath in... and out.',
          duration: 4000
        },
        {
          id: 'checkin',
          text: 'Small check-in - do you feel a little more here? A little more solid?',
          type: 'feedback',
          options: [
            { label: '👍 Yes', value: 'positive' },
            { label: '👎 Not Really', value: 'negative' }
          ]
        },
        {
          id: 'closing',
          text_positive: "Good. That grounding technique! You can use that in the exam hall if you start feeling spaced out. We'll come back to that.",
          text_negative: "That's okay. Sometimes it takes a few tries. We can do it again later if needed.",
          duration: 4000
        }
      ]
    };

    // Session Data Storage
    const sessionData = {
      comfortableLanguages: '',
      preferredColor: '',
      emergencyContact: '',
      goals: '',
      exposureLevel: 'gentle',
      startTime: null,
      conversationHistory: []
    };

    // Conversation Flow
    const conversationFlow = {
      welcome: {
        text: "Welcome to Virtual Lecter's Destressing Experience. We'll practice safely in a virtual classroom. Before we begin, I will ask a few questions to personalize your experience. Are you ready to start?",
        nextStep: 'language',
        saveAs: null
      },
      language: {
        text: "Which languages are you comfortable with? Feel free to mention one or more.",
        nextStep: 'color',
        saveAs: 'comfortableLanguages'
      },
      color: {
        text: "What is your preferred color for the virtual environment?",
        nextStep: 'emergencyContact',
        saveAs: 'preferredColor'
      },
      emergencyContact: {
        text: "For your safety, please provide the name and phone number of an emergency contact person.",
        nextStep: 'goals',
        saveAs: 'emergencyContact'
      },
      goals: {
        text: "What would you like to work on today? For example, 'reduce anxiety when presenting' or 'feel comfortable in class'.",
        nextStep: 'complete',
        saveAs: 'goals'
      },
      complete: {
        text: "Thank you for sharing. We're now ready to begin your virtual classroom experience. Are you ready to enter the virtual environment?",
        nextStep: 'vr',
        saveAs: null
      }
    };

    // DOM Elements
    const vrOverlay = document.getElementById('vrOverlay');
    const manualInputOverlay = document.getElementById('manualInputOverlay');
    const conversationDisplay = document.getElementById('conversationDisplay');
    const liveCaption = document.getElementById('liveCaption');
    const voiceButton = document.getElementById('voiceButton');
    const voiceButtonText = document.getElementById('voiceButtonText');
    const skipButton = document.getElementById('skipButton');
    const permissionInstructions = document.getElementById('permissionInstructions');
    const statusMessage = document.getElementById('statusMessage');
    const manualForm = document.getElementById('manualForm');
    const backButton = document.getElementById('backButton');

    // Voice Recognition
    let recognition = null;
    let currentStep = 'welcome';
    let isListening = false;
    let isSpeaking = false;
    let timeoutId = null;
    let restartCount = 0;
    const MAX_RESTARTS = 3;
    let speechSynthesis = window.speechSynthesis;

    // Initialize Speech Recognition
    function initSpeechRecognition() {
      if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
        alert('Speech recognition is not supported in your browser. Please use Chrome, Edge, or Safari.');
        return false;
      }

      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      recognition = new SpeechRecognition();
      recognition.continuous = false;
      recognition.interimResults = true;
      recognition.lang = 'en-US';

      recognition.onstart = () => {
        isListening = true;
        updateButtonState('listening');
        liveCaption.textContent = 'Listening...';
        
        // Set 60-second timeout
        timeoutId = setTimeout(() => {
          if (isListening) {
            console.log('⏰ 60-second timeout reached');
            recognition.stop();
            updateStatus('No speech detected. Click the button to try again.');
          }
        }, 60000);
      };

      recognition.onresult = (event) => {
        let interimTranscript = '';
        let finalTranscript = '';

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript;
          } else {
            interimTranscript += transcript;
          }
        }

        liveCaption.textContent = interimTranscript || finalTranscript || 'Listening...';

        if (finalTranscript) {
          clearTimeout(timeoutId);
          console.log('✅ Final transcript received:', finalTranscript);
          restartCount = 0; // Reset restart count on successful recognition
          handleUserResponse(finalTranscript.trim());
        }
      };

      recognition.onerror = (event) => {
        clearTimeout(timeoutId);
        isListening = false;
        console.log('❌ Recognition error:', event.error);
        
        if (event.error === 'not-allowed' || event.error === 'permission-denied') {
          permissionInstructions.classList.add('show');
          updateStatus('Microphone permission denied. Please allow access.');
          updateButtonState('ready');
        } else if (event.error === 'no-speech') {
          console.log('⚠️ No speech detected. Restart count:', restartCount);
          if (restartCount < MAX_RESTARTS) {
            restartCount++;
            updateStatus(`No speech detected. Auto-restarting (${restartCount}/${MAX_RESTARTS})...`);
            setTimeout(() => {
              if (!isSpeaking) {
                console.log('🔄 Auto-restarting recognition');
                startListening();
              }
            }, 1000);
          } else {
            handleRecognitionFailure();
          }
        } else {
          updateStatus(`Error: ${event.error}. Click to try again.`);
          updateButtonState('ready');
        }
      };

      recognition.onend = () => {
        clearTimeout(timeoutId);
        isListening = false;
        console.log('🛑 Recognition ended');
        if (voiceButton.classList.contains('listening')) {
          updateButtonState('ready');
        }
      };

      return true;
    }

    // Handle Recognition Failure
    function handleRecognitionFailure() {
      console.log('🚫 Maximum restart attempts reached');
      updateStatus('Having trouble hearing you. Please click the button to try again or use manual input.');
      updateButtonState('ready');
      restartCount = 0;
    }

    // Speak Text with Bulletproof Callback System
    function speak(text, callback) {
      console.log('🔊 Speaking:', text.substring(0, 50) + '...');
      
      isSpeaking = true;
      speechSynthesis.cancel();
      
      const utterance = new SpeechSynthesisUtterance(text);
      utterance.rate = 0.9;
      utterance.pitch = 1;
      utterance.volume = 1;
      
      // Calculate estimated duration (roughly 150ms per word + 1 second buffer)
      const wordCount = text.split(' ').length;
      const estimatedDuration = (wordCount * 150) + 1000;
      console.log('⏱️ Estimated speech duration:', estimatedDuration + 'ms');
      
      let callbackExecuted = false;
      const executeCallback = () => {
        if (!callbackExecuted) {
          callbackExecuted = true;
          isSpeaking = false;
          console.log('✅ Speech finished. Waiting 3 seconds before callback...');
          
          // CRITICAL FIX: 3-second delay after speech ends
          setTimeout(() => {
            console.log('🎤 3-second delay complete - executing callback');
            if (callback) {
              callback();
            }
          }, 3000);
        }
      };
      
      // Primary callback: utterance.onend
      utterance.onend = () => {
        console.log('✅ Speech ended (onend)');
        executeCallback();
      };
      
      // Error fallback
      utterance.onerror = (event) => {
        console.log('⚠️ Speech error:', event.error);
        executeCallback();
      };
      
      // Fallback 1: Timeout based on estimated duration
      const fallbackTimeout = setTimeout(() => {
        console.log('⏰ Fallback timeout triggered');
        executeCallback();
      }, estimatedDuration);
      
      // Fallback 2: Safety net if onend doesn't fire within 1 second after estimated time
      const safetyTimeout = setTimeout(() => {
        console.log('🚨 Safety timeout triggered');
        executeCallback();
      }, estimatedDuration + 1000);
      
      utterance.onstart = () => {
        console.log('🎬 Speech started');
      };
      
      try {
        speechSynthesis.speak(utterance);
        
        // Additional safety: If speechSynthesis fails to speak, trigger callback immediately
        setTimeout(() => {
          if (speechSynthesis.speaking === false && !callbackExecuted) {
            console.log('❌ Speech failed to start - triggering callback immediately');
            executeCallback();
          }
        }, 100);
      } catch (error) {
        console.error('❌ Speech synthesis error:', error);
        executeCallback();
      }
    }

    // Add Message to Conversation
    function addMessage(text, type) {
      const messageDiv = document.createElement('div');
      messageDiv.className = `message ${type}`;
      messageDiv.textContent = text;
      conversationDisplay.appendChild(messageDiv);
      conversationDisplay.scrollTop = conversationDisplay.scrollHeight;
      
      sessionData.conversationHistory.push({ type, text, timestamp: new Date().toISOString() });
    }

    // Update Button State
    function updateButtonState(state) {
      voiceButton.className = '';
      
      switch (state) {
        case 'ready':
          voiceButton.classList.remove('listening', 'processing');
          voiceButtonText.textContent = currentStep === 'welcome' ? 'Start Voice Conversation' : 'Continue';
          voiceButton.disabled = false;
          break;
        case 'listening':
          voiceButton.classList.add('listening');
          voiceButtonText.textContent = 'LISTENING...';
          voiceButton.disabled = false;
          break;
        case 'processing':
          voiceButton.classList.add('processing');
          voiceButtonText.textContent = 'PROCESSING...';
          voiceButton.disabled = true;
          break;
        case 'speaking':
          voiceButton.classList.add('processing');
          voiceButtonText.textContent = 'SPEAKING...';
          voiceButton.disabled = true;
          break;
      }
    }

    // Update Status Message
    function updateStatus(message) {
      statusMessage.textContent = message;
      setTimeout(() => {
        statusMessage.textContent = '';
      }, 5000);
    }

    // Handle User Response
    function handleUserResponse(response) {
      updateButtonState('processing');
      liveCaption.textContent = `Got it: "${response}"`;
      addMessage(response, 'user');

      const step = conversationFlow[currentStep];
      
      // Save response if needed
      if (step.saveAs) {
        sessionData[step.saveAs] = response;
      }

      // Move to next step
      setTimeout(() => {
        proceedToNextStep();
      }, 1000);
    }

    // Proceed to Next Step
    function proceedToNextStep() {
      const step = conversationFlow[currentStep];
      const nextStep = step.nextStep;

      if (nextStep === 'vr') {
        // Complete conversation
        updateButtonState('speaking');
        speak('Great! Entering the virtual classroom now.', () => {
          completeSession();
        });
      } else {
        currentStep = nextStep;
        const nextStepData = conversationFlow[nextStep];
        
        updateButtonState('speaking');
        addMessage(nextStepData.text, 'system');
        
        // Speak and auto-start listening after (includes 3s delay)
        speak(nextStepData.text, () => {
          liveCaption.textContent = 'Ready to listen...';
          startListening();
        });
      }
    }

    // Start Listening
    function startListening() {
      if (isSpeaking) {
        console.log('⚠️ Cannot start listening while speaking');
        return;
      }
      
      console.log('👂 Starting voice recognition');
      try {
        recognition.start();
        console.log('✅ Recognition started');
      } catch (error) {
        console.error('❌ Recognition start error:', error);
        
        // If already running, this is expected
        if (error.message && error.message.includes('already started')) {
          console.log('ℹ️ Recognition already running');
        } else {
          updateStatus('Error starting recognition. Please try again.');
          updateButtonState('ready');
        }
      }
    }

    // Start Voice Conversation
    function startVoiceConversation() {
      console.log('🎬 Starting conversation');
      
      if (!recognition && !initSpeechRecognition()) {
        return;
      }

      if (isListening) {
        recognition.stop();
        return;
      }

      // If first time, speak welcome message and auto-start listening
      if (currentStep === 'welcome') {
        sessionData.startTime = new Date().toISOString();
        updateButtonState('speaking');
        const welcomeText = conversationFlow.welcome.text;
        addMessage(welcomeText, 'system');
        
        // Speak with callback to auto-start listening (includes 3s delay)
        speak(welcomeText, () => {
          liveCaption.textContent = 'Ready to listen...';
          startListening();
        });
      } else {
        // Start listening immediately
        startListening();
      }
    }

    // Complete Session (Onboarding)
    function completeSession() {
      console.log('Session Data:', sessionData);
      vrOverlay.classList.add('hidden');
      updateStatus('Welcome to your virtual classroom!');
      
      // Show "Begin Guided Session" button after 2 seconds
      setTimeout(() => {
        showBeginSessionButton();
      }, 2000);
    }

    // Show Begin Session Button
    function showBeginSessionButton() {
      const beginBtn = document.getElementById('beginSessionBtn');
      if (beginBtn) {
        beginBtn.setAttribute('visible', 'true');
        beginBtn.setAttribute('animation', 'property: scale; from: 0.5 0.5 0.5; to: 1 1 1; dur: 300; easing: easeOutQuad');
        console.log('✅ Begin Session button shown');
      }
    }

    // Start Guided Session
    function startGuidedSession() {
      console.log('🎬 Starting guided session');
      sessionState.active = true;
      sessionState.startTime = new Date().toISOString();
      sessionState.phaseStartTime = new Date().toISOString();
      
      // Hide begin button
      const beginBtn = document.getElementById('beginSessionBtn');
      if (beginBtn) {
        beginBtn.setAttribute('visible', 'false');
      }
      
      // Show dialogue panel
      const dialoguePanel = document.getElementById('dialoguePanel');
      if (dialoguePanel) {
        dialoguePanel.setAttribute('visible', 'true');
        dialoguePanel.setAttribute('animation', 'property: opacity; from: 0; to: 1; dur: 300');
      }
      
      // Start grounding phase
      startGroundingPhase();
    }

    // Start Grounding Phase
    function startGroundingPhase() {
      console.log('🧘 Starting grounding phase');
      sessionState.currentPhase = 'grounding';
      sessionState.currentStep = 0;
      processGroundingStep();
    }

    // Process Grounding Step
    function processGroundingStep() {
      const step = groundingPhase.steps[sessionState.currentStep];
      console.log(`📍 Step ${sessionState.currentStep + 1}/${groundingPhase.steps.length}:`, step.id);
      
      const dialogueText = document.getElementById('dialogueText');
      const visualText = document.getElementById('visualText');
      const feedbackButtons = document.getElementById('feedbackButtons');
      const continueBtn = document.getElementById('continueBtn');
      
      // Hide all interactive elements first
      if (feedbackButtons) feedbackButtons.setAttribute('visible', 'false');
      if (continueBtn) continueBtn.setAttribute('visible', 'false');
      if (visualText) visualText.setAttribute('visible', 'false');
      
      // Update dialogue text
      let displayText = step.text || '';
      if (step.prompt) {
        displayText += '\n\n' + step.prompt;
      }
      
      if (dialogueText) {
        dialogueText.setAttribute('value', displayText);
      }
      
      // Show visual if present (like "5-4-3-2-1")
      if (step.visual && visualText) {
        visualText.setAttribute('value', step.visual);
        visualText.setAttribute('visible', 'true');
      }
      
      // Speak the text
      speak(step.text, () => {
        // After speaking, handle step type
        if (step.type === 'feedback') {
          // Show feedback buttons
          if (feedbackButtons) {
            feedbackButtons.setAttribute('visible', 'true');
            feedbackButtons.setAttribute('animation', 'property: scale; from: 0.8 0.8 0.8; to: 1 1 1; dur: 300');
          }
        } else if (step.duration) {
          // Timed step - auto-advance after duration
          setTimeout(() => {
            advanceGroundingStep();
          }, step.duration);
        } else {
          // Manual advance with continue button
          if (continueBtn) {
            continueBtn.setAttribute('visible', 'true');
            continueBtn.setAttribute('animation', 'property: scale; from: 0.8 0.8 0.8; to: 1 1 1; dur: 300');
          }
        }
      });
    }

    // Advance Grounding Step
    function advanceGroundingStep() {
      sessionState.currentStep++;
      
      if (sessionState.currentStep < groundingPhase.steps.length) {
        processGroundingStep();
      } else {
        completeGroundingPhase();
      }
    }

    // Handle Feedback Response
    function handleFeedbackResponse(isPositive) {
      console.log('📝 Feedback:', isPositive ? 'Positive' : 'Negative');
      sessionState.responses.groundingFeedback = isPositive ? 'positive' : 'negative';
      
      // Hide feedback buttons
      const feedbackButtons = document.getElementById('feedbackButtons');
      if (feedbackButtons) {
        feedbackButtons.setAttribute('visible', 'false');
      }
      
      // Update text based on response
      const step = groundingPhase.steps[sessionState.currentStep];
      const responseText = isPositive ? step.text_positive : step.text_negative;
      
      const dialogueText = document.getElementById('dialogueText');
      if (dialogueText) {
        dialogueText.setAttribute('value', responseText);
      }
      
      // Speak response and show continue button
      speak(responseText, () => {
        const continueBtn = document.getElementById('continueBtn');
        if (continueBtn) {
          continueBtn.setAttribute('visible', 'true');
          continueBtn.setAttribute('animation', 'property: scale; from: 0.8 0.8 0.8; to: 1 1 1; dur: 300');
        }
      });
    }

    // Complete Grounding Phase
    function completeGroundingPhase() {
      console.log('✅ Grounding phase complete');
      
      const dialogueText = document.getElementById('dialogueText');
      if (dialogueText) {
        dialogueText.setAttribute('value', 'Phase 1 complete. Phase 2 will continue with building rapport and understanding your specific concerns.');
      }
      
      speak('Phase 1 complete. Phase 2 will continue with building rapport and understanding your specific concerns.', () => {
        // Show continue button to return to classroom
        setTimeout(() => {
          const dialoguePanel = document.getElementById('dialoguePanel');
          if (dialoguePanel) {
            dialoguePanel.setAttribute('visible', 'false');
          }
          
          // Show begin button again for restart
          showBeginSessionButton();
        }, 5000);
      });
    }

    // Switch to Manual Input
    function switchToManualInput() {
      vrOverlay.style.display = 'none';
      manualInputOverlay.classList.add('active');
    }

    // Back to Voice Mode
    function backToVoiceMode() {
      manualInputOverlay.classList.remove('active');
      vrOverlay.style.display = 'flex';
    }

    // Handle Manual Form Submit
    function handleManualSubmit(e) {
      e.preventDefault();
      
      sessionData.comfortableLanguages = document.getElementById('manualLanguages').value;
      sessionData.preferredColor = document.getElementById('manualColor').value;
      sessionData.emergencyContact = document.getElementById('manualContact').value;
      sessionData.goals = document.getElementById('manualGoals').value;
      sessionData.startTime = new Date().toISOString();

      console.log('Session Data (Manual):', sessionData);
      
      manualInputOverlay.classList.remove('active');
      vrOverlay.classList.add('hidden');
      
      speak('Welcome to your virtual classroom. Take your time to explore.', null);
    }

    // Event Listeners
    voiceButton.addEventListener('click', startVoiceConversation);
    skipButton.addEventListener('click', switchToManualInput);
    backButton.addEventListener('click', backToVoiceMode);
    manualForm.addEventListener('submit', handleManualSubmit);

    // Setup VR Click Handlers
    function setupVRClickHandlers() {
      // Begin Session Button
      const beginBtn = document.getElementById('beginSessionBtn');
      if (beginBtn) {
        beginBtn.addEventListener('click', () => {
          console.log('🖱️ Begin Session clicked');
          startGuidedSession();
        });
        
        // Hover effect
        beginBtn.addEventListener('mouseenter', () => {
          beginBtn.setAttribute('scale', '1.1 1.1 1.1');
        });
        beginBtn.addEventListener('mouseleave', () => {
          beginBtn.setAttribute('scale', '1 1 1');
        });
      }
      
      // Yes Button
      const yesBtn = document.getElementById('yesBtn');
      if (yesBtn) {
        yesBtn.addEventListener('click', () => {
          console.log('🖱️ Yes button clicked');
          handleFeedbackResponse(true);
        });
        
        yesBtn.addEventListener('mouseenter', () => {
          yesBtn.setAttribute('scale', '1.1 1.1 1.1');
        });
        yesBtn.addEventListener('mouseleave', () => {
          yesBtn.setAttribute('scale', '1 1 1');
        });
      }
      
      // No Button
      const noBtn = document.getElementById('noBtn');
      if (noBtn) {
        noBtn.addEventListener('click', () => {
          console.log('🖱️ No button clicked');
          handleFeedbackResponse(false);
        });
        
        noBtn.addEventListener('mouseenter', () => {
          noBtn.setAttribute('scale', '1.1 1.1 1.1');
        });
        noBtn.addEventListener('mouseleave', () => {
          noBtn.setAttribute('scale', '1 1 1');
        });
      }
      
      // Continue Button
      const continueBtn = document.getElementById('continueBtn');
      if (continueBtn) {
        continueBtn.addEventListener('click', () => {
          console.log('🖱️ Continue button clicked');
          advanceGroundingStep();
        });
        
        continueBtn.addEventListener('mouseenter', () => {
          continueBtn.setAttribute('scale', '1.1 1.1 1.1');
        });
        continueBtn.addEventListener('mouseleave', () => {
          continueBtn.setAttribute('scale', '1 1 1');
        });
      }
    }

    // Initialize
    document.addEventListener('DOMContentLoaded', () => {
      console.log('VR Classroom Exposure Therapy Initialized');
      
      // Wait for A-Frame to load, then setup VR handlers
      const scene = document.querySelector('a-scene');
      if (scene) {
        if (scene.hasLoaded) {
          setupVRClickHandlers();
        } else {
          scene.addEventListener('loaded', () => {
            setupVRClickHandlers();
          });
        }
      }
    });
  </script>
</body>
</html>

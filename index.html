<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>VR Classroom Exposure Therapy</title>
  <script src="https://aframe.io/releases/1.4.2/aframe.min.js"></script>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
      overflow: hidden;
      background: linear-gradient(135deg, #E8F4F8 0%, #F0F8FA 100%);
    }

    /* VR Overlay */
    #vrOverlay {
      position: fixed;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      backdrop-filter: blur(8px);
      background: rgba(232, 244, 248, 0.85);
      display: flex;
      align-items: center;
      justify-content: center;
      z-index: 1000;
      transition: opacity 0.5s ease, visibility 0.5s ease;
    }

    #vrOverlay.hidden {
      opacity: 0;
      visibility: hidden;
      pointer-events: none;
    }

    .overlay-card {
      max-width: 420px;
      width: 90%;
      background: white;
      border-radius: 16px;
      padding: 24px;
      box-shadow: 0 10px 40px rgba(19, 52, 59, 0.15);
      animation: slideUp 0.4s ease;
    }

    @keyframes slideUp {
      from {
        opacity: 0;
        transform: translateY(20px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    .overlay-card h1 {
      font-size: 20px;
      color: rgba(19, 52, 59, 1);
      margin-bottom: 8px;
      font-weight: 600;
    }

    .overlay-card p {
      font-size: 13px;
      color: rgba(19, 52, 59, 0.7);
      margin-bottom: 20px;
      line-height: 1.5;
    }

    /* Conversation Display */
    #conversationDisplay {
      max-height: 200px;
      overflow-y: auto;
      margin-bottom: 16px;
      padding: 12px;
      background: rgba(232, 244, 248, 0.5);
      border-radius: 8px;
    }

    .message {
      margin-bottom: 10px;
      padding: 8px;
      border-radius: 6px;
      font-size: 12px;
      line-height: 1.4;
    }

    .message.system {
      background: rgba(33, 128, 141, 0.1);
      border-left: 3px solid rgba(33, 128, 141, 1);
    }

    .message.user {
      background: rgba(94, 82, 64, 0.1);
      border-left: 3px solid rgba(94, 82, 64, 0.8);
    }

    .message strong {
      font-weight: 600;
      margin-right: 4px;
    }

    /* Voice Button */
    #voiceBtn {
      width: 100%;
      padding: 12px 20px;
      font-size: 14px;
      font-weight: 600;
      border: none;
      border-radius: 8px;
      background: #FF6B6B;
      color: white;
      cursor: pointer;
      transition: all 0.2s ease;
      margin-bottom: 12px;
    }

    #voiceBtn:hover {
      background: #EE5A5A;
      transform: translateY(-1px);
    }

    #voiceBtn.listening {
      background: #51CF66;
      animation: pulse 1.5s infinite;
    }

    @keyframes pulse {
      0%, 100% { opacity: 1; }
      50% { opacity: 0.7; }
    }

    #voiceStatus {
      text-align: center;
      font-size: 12px;
      color: rgba(19, 52, 59, 0.6);
      margin-bottom: 16px;
      min-height: 20px;
    }

    #skipBtn {
      width: 100%;
      padding: 10px 20px;
      font-size: 13px;
      font-weight: 500;
      border: 1px solid rgba(94, 82, 64, 0.2);
      border-radius: 8px;
      background: rgba(94, 82, 64, 0.05);
      color: rgba(19, 52, 59, 1);
      cursor: pointer;
      transition: all 0.2s ease;
    }

    #skipBtn:hover {
      background: rgba(94, 82, 64, 0.1);
    }

    /* Manual Input Overlay */
    #manualOverlay {
      position: fixed;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      backdrop-filter: blur(8px);
      background: rgba(232, 244, 248, 0.85);
      display: none;
      align-items: center;
      justify-content: center;
      z-index: 1001;
    }

    #manualOverlay.active {
      display: flex;
    }

    .manual-form {
      max-width: 420px;
      width: 90%;
      background: white;
      border-radius: 16px;
      padding: 24px;
      box-shadow: 0 10px 40px rgba(19, 52, 59, 0.15);
      max-height: 80vh;
      overflow-y: auto;
    }

    .manual-form h2 {
      font-size: 18px;
      color: rgba(19, 52, 59, 1);
      margin-bottom: 16px;
    }

    .form-group {
      margin-bottom: 16px;
    }

    .form-group label {
      display: block;
      font-size: 12px;
      font-weight: 600;
      color: rgba(19, 52, 59, 1);
      margin-bottom: 6px;
    }

    .form-group input,
    .form-group textarea {
      width: 100%;
      padding: 10px 12px;
      font-size: 13px;
      border: 1px solid rgba(94, 82, 64, 0.2);
      border-radius: 6px;
      font-family: inherit;
    }

    .form-group textarea {
      resize: vertical;
      min-height: 80px;
    }

    .button-group {
      display: flex;
      gap: 10px;
      margin-top: 20px;
    }

    .button-group button {
      flex: 1;
      padding: 10px 20px;
      font-size: 13px;
      font-weight: 600;
      border: none;
      border-radius: 8px;
      cursor: pointer;
      transition: all 0.2s ease;
    }

    .btn-secondary {
      background: rgba(94, 82, 64, 0.1);
      color: rgba(19, 52, 59, 1);
    }

    .btn-primary {
      background: rgba(33, 128, 141, 1);
      color: white;
    }

    .btn-primary:hover {
      background: rgba(29, 116, 128, 1);
    }
  </style>
</head>
<body>
  <!-- Voice Conversation Overlay -->
  <div id="vrOverlay">
    <div class="overlay-card">
      <h1>üéì Virtual Lecter's Destressing Experience</h1>
      <p>We'll collect some information through voice conversation before starting.</p>
      
      <div id="conversationDisplay">
        <div class="message system">
          <strong>System:</strong> Welcome to Virtual Lecter's Destressing Experience
        </div>
      </div>

      <button id="voiceBtn">üé§ Click to Start Voice Conversation</button>
      <div id="voiceStatus">Ready to begin</div>
      <button id="skipBtn">Skip Voice Mode (Use Manual Input)</button>
    </div>
  </div>

  <!-- Manual Input Overlay -->
  <div id="manualOverlay">
    <div class="manual-form">
      <h2>Basic Information</h2>
      <p style="font-size: 12px; color: rgba(19, 52, 59, 0.7); margin-bottom: 16px;">
        Let's collect some information to personalize your experience.
      </p>

      <div class="form-group">
        <label>Comfortable Languages</label>
        <input type="text" id="manualLanguages" placeholder="e.g., English, Hindi">
      </div>

      <div class="form-group">
        <label>Preferred Color</label>
        <input type="text" id="manualColor" placeholder="e.g., Blue, Green">
      </div>

      <div class="form-group">
        <label>Emergency Contact (Name and Phone)</label>
        <textarea id="manualEmergency" placeholder="e.g., John Doe, +91-1234567890"></textarea>
      </div>

      <div class="form-group">
        <label>Session Goals</label>
        <textarea id="manualGoals" placeholder="What would you like to work on today?"></textarea>
      </div>

      <div class="button-group">
        <button class="btn-secondary" onclick="closeManualInput()">Back</button>
        <button class="btn-primary" onclick="submitManualData()">Start VR Experience</button>
      </div>
    </div>
  </div>

  <!-- A-Frame VR Scene -->
  <a-scene>
    <a-sky color="#E8F4F8"></a-sky>
    
    <a-entity light="type: ambient; color: #BBB; intensity: 0.8"></a-entity>
    <a-entity light="type: directional; color: #FFF; intensity: 0.6" position="5 10 7"></a-entity>
    <a-entity light="type: point; color: #FFF; intensity: 0.4" position="0 3.5 0"></a-entity>

    <a-entity 
      gltf-model="https://raw.githubusercontent.com/o-meiuqer-o/classroom_VR4ET/main/assets/models/classroom.glb"
      position="0 0 0"
      rotation="0 180 0"
      scale="1.15 1.15 1.15"
      shadow="cast: true; receive: true">
    </a-entity>

    <a-entity id="rig" position="-0.5 0.6 1.8">
      <a-camera look-controls wasd-controls>
        <a-cursor color="#21808D"></a-cursor>
      </a-camera>
    </a-entity>
  </a-scene>

  <script>
    console.log('VR Classroom Exposure Therapy Initialized');

    // Session data
    const sessionData = {
      comfortableLanguages: '',
      preferredColor: '',
      emergencyContact: '',
      goals: '',
      startTime: new Date()
    };

    // Speech APIs
    let recognition = null;
    let synthesis = window.speechSynthesis;
    let voices = [];
    let recognitionTimeout = null;
    let isSpeaking = false;
    let restartCount = 0;
    const MAX_RESTARTS = 3;

    // Load voices
    if (synthesis) {
      synthesis.onvoiceschanged = () => {
        voices = synthesis.getVoices();
        console.log('‚úÖ Voices loaded:', voices.length);
      };
    }

    // Setup speech recognition
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      recognition = new SpeechRecognition();
      recognition.continuous = false;
      recognition.interimResults = true;
      recognition.maxAlternatives = 3;
      recognition.lang = 'en-US';
      console.log('‚úÖ Speech Recognition initialized');
    } else {
      console.log('‚ùå Speech Recognition not available');
    }

    // Conversation state
    let conversationState = {
      step: 'welcome',
      isListening: false,
      responses: {},
      hasReceivedSpeech: false
    };

    // Conversation flow
    const conversationFlow = {
      welcome: {
        text: "Welcome to Virtual Lecter's Destressing Experience. We'll practice safely in a virtual classroom. Before we begin, I will ask a few questions to personalize your experience. Are you ready to start?",
        nextStep: 'language',
        expectedResponses: ['yes', 'okay', 'sure', 'ready', 'thank you']
      },
      language: {
        text: "Which languages are you comfortable with? Feel free to mention one or more.",
        nextStep: 'color',
        saveAs: 'comfortableLanguages'
      },
      color: {
        text: "What is your preferred color for the virtual environment?",
        nextStep: 'emergencyContact',
        saveAs: 'preferredColor'
      },
      emergencyContact: {
        text: "For your safety, please provide the name and phone number of an emergency contact person.",
        nextStep: 'goals',
        saveAs: 'emergencyContact'
      },
      goals: {
        text: "What would you like to work on today? For example, 'reduce anxiety when presenting' or 'feel comfortable in class'.",
        nextStep: 'complete',
        saveAs: 'goals'
      },
      complete: {
        text: "Thank you for sharing. We're now ready to begin your virtual classroom experience. Are you ready to enter the virtual environment?",
        nextStep: 'vr',
        expectedResponses: ['yes', 'ready', 'let\'s go', 'thank you']
      }
    };

    // Enhanced speak function with 3-second delay
    function speak(text, callback) {
      console.log('üîä Speaking:', text.substring(0, 50) + '...');
      
      if (!synthesis) {
        console.log('‚ùå Speech synthesis not available');
        if (callback) callback();
        return;
      }

      synthesis.cancel();
      isSpeaking = true;
      restartCount = 0;

      const utterance = new SpeechSynthesisUtterance(text);
      utterance.rate = 0.85;
      utterance.pitch = 1;
      utterance.volume = 1;
      utterance.lang = 'en-US';

      if (voices.length > 0) {
        const preferredVoice = voices.find(v => 
          v.lang.includes('en') && (v.name.includes('Female') || v.name.includes('Zira'))
        ) || voices[0];
        utterance.voice = preferredVoice;
      }

      const wordCount = text.split(' ').length;
      const estimatedDuration = (wordCount * 150) + 3000;
      console.log('‚è±Ô∏è Estimated speech duration:', estimatedDuration + 'ms');

      let callbackExecuted = false;
      const executeCallback = () => {
        if (!callbackExecuted && callback) {
          callbackExecuted = true;
          isSpeaking = false;
          console.log('‚úÖ Speech ended - waiting 3s before starting recognition');
          
          setTimeout(() => {
            console.log('üé§ Now safe to start listening');
            callback();
          }, 3000);
        }
      };

      utterance.onstart = () => {
        console.log('üîä Speech started');
      };

      utterance.onend = () => {
        console.log('‚úÖ Speech onend event fired');
        executeCallback();
      };

      utterance.onerror = (e) => {
        console.error('‚ùå Speech error:', e);
        isSpeaking = false;
        executeCallback();
      };

      setTimeout(() => {
        if (!callbackExecuted) {
          console.log('‚è±Ô∏è Fallback timeout triggered');
          executeCallback();
        }
      }, estimatedDuration + 3000);

      addMessage('System', text);
      synthesis.speak(utterance);
    }

    // Add message to conversation display
    function addMessage(sender, text) {
      const display = document.getElementById('conversationDisplay');
      const messageDiv = document.createElement('div');
      messageDiv.className = `message ${sender.toLowerCase()}`;
      messageDiv.innerHTML = `<strong>${sender}:</strong> ${text}`;
      display.appendChild(messageDiv);
      display.scrollTop = display.scrollHeight;
    }

    // Request microphone permission
    async function requestMicPermission() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        stream.getTracks().forEach(track => track.stop());
        console.log('‚úÖ Microphone permission granted');
        return true;
      } catch (error) {
        console.error('‚ùå Microphone permission denied:', error);
        alert('Microphone access is required for voice conversation. Please allow access and try again.');
        return false;
      }
    }

    // Start conversation
    async function startConversation() {
      const voiceBtn = document.getElementById('voiceBtn');
      const statusEl = document.getElementById('voiceStatus');

      console.log('üé¨ Starting conversation from step:', conversationState.step);

      const currentStep = conversationState.step || 'welcome';
      const currentFlow = conversationFlow[currentStep];

      if (currentStep !== 'welcome' && Object.keys(conversationState.responses).length > 0) {
        console.log('üîÑ Resuming conversation');
        voiceBtn.textContent = 'üîä Resuming...';
        statusEl.textContent = 'Resuming conversation';

        speak("Let's continue from where we left off. " + currentFlow.text, () => {
          startListening(currentStep);
        });
        return;
      }

      voiceBtn.textContent = 'üé§ Requesting microphone...';
      statusEl.textContent = 'Please allow microphone access';

      const hasPermission = await requestMicPermission();
      if (!hasPermission) {
        voiceBtn.textContent = 'üé§ Click to Start Voice Conversation';
        statusEl.textContent = '‚ùå Microphone access denied';
        return;
      }

      voiceBtn.textContent = 'üîä Speaking...';
      statusEl.textContent = 'System is speaking';

      speak(conversationFlow.welcome.text, () => {
        console.log('üé§ Speech completed, starting listening');
        startListening('welcome');
      });
    }

    // Start listening
    function startListening(step) {
      if (!recognition) {
        alert('Speech recognition not supported. Please use manual input.');
        return;
      }

      if (isSpeaking) {
        console.log('‚ö†Ô∏è System still speaking, delaying recognition start');
        setTimeout(() => startListening(step), 500);
        return;
      }

      console.log('üëÇ Starting to listen for step:', step);
      
      const voiceBtn = document.getElementById('voiceBtn');
      const statusEl = document.getElementById('voiceStatus');

      conversationState.step = step;
      conversationState.isListening = true;
      conversationState.hasReceivedSpeech = false;

      voiceBtn.textContent = 'üé§ Listening...';
      voiceBtn.classList.add('listening');
      statusEl.textContent = 'üî¥ LISTENING - Speak now (60s window)';
      statusEl.style.color = '#FF6B6B';
      statusEl.style.fontWeight = 'bold';

      if (recognitionTimeout) clearTimeout(recognitionTimeout);

      recognition.onstart = () => {
        console.log('‚úÖ Recognition started');
        recognitionTimeout = setTimeout(() => {
          if (conversationState.isListening && !conversationState.hasReceivedSpeech) {
            console.log('‚è±Ô∏è 60s timeout - restarting');
            recognition.stop();
          }
        }, 60000);
      };

      recognition.onspeechstart = () => {
        console.log('üó£Ô∏è Speech detected');
        conversationState.hasReceivedSpeech = true;
        if (recognitionTimeout) clearTimeout(recognitionTimeout);
        statusEl.textContent = '‚úÖ HEARING YOU - Keep speaking';
        statusEl.style.color = '#51CF66';
      };

      recognition.onresult = (event) => {
        let finalTranscript = '';
        
        for (let i = event.resultIndex; i < event.results.length; i++) {
          if (event.results[i].isFinal) {
            finalTranscript += event.results[i][0].transcript;
          } else {
            const interim = event.results[i][0].transcript;
            statusEl.textContent = `üìù Hearing: "${interim}"`;
          }
        }

        if (finalTranscript) {
          const transcript = finalTranscript.toLowerCase().trim();
          console.log('‚úÖ Final result:', transcript);

          if (recognitionTimeout) clearTimeout(recognitionTimeout);
          conversationState.isListening = false;

          voiceBtn.classList.remove('listening');
          voiceBtn.textContent = '‚úÖ Got it!';
          statusEl.textContent = `You said: "${transcript}"`;
          statusEl.style.color = '#51CF66';

          addMessage('You', transcript);
          processResponse(transcript, step);
        }
      };

      recognition.onend = () => {
        console.log('üõë Recognition ended');
        
        if (recognitionTimeout) clearTimeout(recognitionTimeout);

        if (conversationState.isListening && !conversationState.hasReceivedSpeech) {
          restartCount++;
          
          if (restartCount <= MAX_RESTARTS) {
            console.log(`üîÑ Auto-restarting recognition (attempt ${restartCount}/${MAX_RESTARTS})`);
            setTimeout(() => {
              try {
                recognition.start();
              } catch (e) {
                console.error('Failed to restart:', e);
                handleRecognitionFailure();
              }
            }, 100);
          } else {
            console.log('‚ö†Ô∏è Max restarts reached - stopping auto-restart');
            handleRecognitionFailure();
          }
        }
      };

      recognition.onerror = (event) => {
        console.error('‚ùå Recognition error:', event.error);
        
        if (event.error === 'no-speech') {
          return;
        }

        handleRecognitionFailure();
      };

      try {
        recognition.start();
        console.log('‚úÖ Recognition start called');
      } catch (error) {
        console.error('‚ùå Failed to start recognition:', error);
        alert('Could not start voice recognition. Please try again.');
      }
    }

    function handleRecognitionFailure() {
      const voiceBtn = document.getElementById('voiceBtn');
      const statusEl = document.getElementById('voiceStatus');
      voiceBtn.classList.remove('listening');
      voiceBtn.textContent = 'üîÑ Click to Resume';
      statusEl.textContent = 'No speech detected - Click to try again';
      statusEl.style.color = '';
      statusEl.style.fontWeight = '';
      conversationState.isListening = false;
      restartCount = 0;
    }

    // Process response
    function processResponse(transcript, step) {
      const currentFlow = conversationFlow[step];
      
      if (currentFlow.saveAs) {
        conversationState.responses[currentFlow.saveAs] = transcript;
      }

      if (currentFlow.nextStep === 'vr') {
        completeConversation();
      } else {
        setTimeout(() => continueConversation(currentFlow.nextStep), 1000);
      }
    }

    // Continue conversation
    function continueConversation(nextStep) {
      const nextFlow = conversationFlow[nextStep];
      const voiceBtn = document.getElementById('voiceBtn');
      const statusEl = document.getElementById('voiceStatus');

      voiceBtn.textContent = 'üîä Speaking...';
      voiceBtn.classList.remove('listening');
      statusEl.textContent = 'System is speaking';
      statusEl.style.color = '';
      statusEl.style.fontWeight = '';

      speak(nextFlow.text, () => {
        startListening(nextStep);
      });
    }

    // Complete conversation
    function completeConversation() {
      Object.assign(sessionData, conversationState.responses);
      
      console.log('‚úÖ Conversation complete:', sessionData);

      const voiceBtn = document.getElementById('voiceBtn');
      const statusEl = document.getElementById('voiceStatus');

      voiceBtn.textContent = '‚úÖ Complete - Starting VR';
      voiceBtn.classList.remove('listening');
      statusEl.textContent = 'Launching virtual classroom...';

      setTimeout(() => {
        document.getElementById('vrOverlay').classList.add('hidden');
        speak('Welcome to the virtual classroom. Take your time and remember you can exit at any moment.');
      }, 2000);
    }

    // Manual input functions
    function showManualInput() {
      document.getElementById('manualOverlay').classList.add('active');
    }

    function closeManualInput() {
      document.getElementById('manualOverlay').classList.remove('active');
    }

    function submitManualData() {
      sessionData.comfortableLanguages = document.getElementById('manualLanguages').value;
      sessionData.preferredColor = document.getElementById('manualColor').value;
      sessionData.emergencyContact = document.getElementById('manualEmergency').value;
      sessionData.goals = document.getElementById('manualGoals').value;

      console.log('‚úÖ Manual data submitted:', sessionData);

      closeManualInput();
      document.getElementById('vrOverlay').classList.add('hidden');
      speak('Thank you. Welcome to the virtual classroom.');
    }

    // Event listeners
    document.getElementById('voiceBtn').addEventListener('click', startConversation);
    document.getElementById('skipBtn').addEventListener('click', showManualInput);
  </script>
</body>
</html>
